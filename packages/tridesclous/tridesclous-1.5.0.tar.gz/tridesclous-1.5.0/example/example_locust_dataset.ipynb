{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *tridesclous* example with locust dataset\n",
    "\n",
    "Here a detail notebook that detail the locust dataset recodring by Christophe Pouzat.\n",
    "\n",
    "This dataset is our classic.\n",
    "It has be analyse yet by several tools in R, Python or C:\n",
    "  * https://github.com/christophe-pouzat/PouzatDetorakisEuroScipy2014\n",
    "  * https://github.com/christophe-pouzat/SortingABigDataSetWithPython\n",
    "  * http://xtof.perso.math.cnrs.fr/locust.html\n",
    "\n",
    "So we can compare the result.\n",
    "\n",
    "The original datasets is here https://zenodo.org/record/21589\n",
    "\n",
    "But we will work on a very small subset on github https://github.com/tridesclous/tridesclous_datasets/tree/master/locust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In *tridesclous*, the spike sorting is done in several step:\n",
    "  * Define the datasource and working path. (class DataIO)\n",
    "  * Construct a *catalogue* (class CatalogueConstructor) on a short chunk of data (for instance 60s)\n",
    "    with several sub step :\n",
    "    * signal pre-processing:\n",
    "      * high pass filter (optional)\n",
    "      * removal of common reference (optional)\n",
    "      * noise estimation (median/mad) on a small chunk\n",
    "      * normalisation = robust z-score\n",
    "    * peak detection\n",
    "    * extract some waveform. Unecessary and impossible to extract them all.\n",
    "    * find rational limit of waveforms (n_left/n_right)\n",
    "    * project theses waveforms in smaller dimention (pca, ...)\n",
    "    * find cluster\n",
    "    * clean with GUI (class CatalogueWindow)\n",
    "    * save centroids (median+mad + first and second derivative)\n",
    "  * Apply the *Peeler* (class Peeler) on the long term signals. With several sub steps:\n",
    "     * same signal preprocessing than before\n",
    "     * find peaks\n",
    "     * find the best cluster in catalogue for each peak\n",
    "     * find the intersample jitter\n",
    "     * remove the oversampled waveforms from the signals until there are not peaks in the signals.\n",
    "     * check with GUI (class PeelerWindow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tridesclous as tdc\n",
    "\n",
    "from tridesclous import DataIO, CatalogueConstructor, Peeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a small dataset\n",
    "\n",
    "trideclous provide some datasets than can be downloaded with **download_dataset**.\n",
    "\n",
    "Note this dataset contains 2 trials in 2 different files. (the original contains more!)\n",
    "\n",
    "Each file is considers as a *segment*. *tridesclous* automatically deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/samuel/Documents/projet/tridesclous/example/locust/locust_trial_01.raw', '/home/samuel/Documents/projet/tridesclous/example/locust/locust_trial_02.raw']\n",
      "{'dtype': 'int16', 'sample_rate': 15000.0, 'total_channel': 4, 'bit_to_microVolt': None}\n"
     ]
    }
   ],
   "source": [
    "#download dataset\n",
    "localdir, filenames, params = tdc.download_dataset(name='locust')\n",
    "print(filenames)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataIO = define datasource and working dir\n",
    "\n",
    "\n",
    "Theses 2 files are in **RawData** format this means binary format with interleaved channels.\n",
    "\n",
    "Our dataset contains 2 segment of 28.8 second each, 4 channels. The sample rate is 15kHz.\n",
    "\n",
    "Note that there is only one channel_group here (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIO <id: 140328470155672> \n",
      "  workdir: tridesclous_locust\n",
      "  sample_rate: 15000.0\n",
      "  total_channel: 4\n",
      "  channel_groups: 0 [ch0 ch1 ch2 ch3]\n",
      "  nb_segment: 2\n",
      "  length: 431548 431548\n",
      "  durations: 28.8 28.8 s.\n"
     ]
    }
   ],
   "source": [
    "#create a DataIO\n",
    "import os, shutil\n",
    "dirname = 'tridesclous_locust'\n",
    "if os.path.exists(dirname):\n",
    "    #remove is already exists\n",
    "    shutil.rmtree(dirname)    \n",
    "dataio = DataIO(dirname=dirname)\n",
    "\n",
    "# feed DataIO\n",
    "dataio.set_data_source(type='RawData', filenames=filenames, **params)\n",
    "print(dataio)\n",
    "\n",
    "#no need to setup the prb with dataio.set_probe_file() or dataio.download_probe()\n",
    "#because it is a tetrode\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatalogueConstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  Signal pre-processing not done yet\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor = CatalogueConstructor(dataio=dataio)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters\n",
    "\n",
    "For a complet description of each params see main documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global params\n",
    "catalogueconstructor.set_global_params(chunksize=1024,mode='dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing filetring normalisation\n",
    "catalogueconstructor.set_preprocessor_params(\n",
    "            common_ref_removal=False,\n",
    "            highpass_freq=300.,\n",
    "            lowpass_freq=5000.,                                             \n",
    "            lostfront_chunksize=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogueconstructor.set_peak_detector_params(\n",
    "            peak_sign='-',\n",
    "            relative_threshold=6.5,\n",
    "            peak_span_ms=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the median and mad of noiseon a small chunk of filtered signals.\n",
    "This compute medians and mad of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2877347 0.8443531 1.6870663 0.5088713]\n",
      "[51.053234 46.69039  57.44741  44.837955]\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor.estimate_signals_noise(seg_num=0, duration=15.)\n",
    "print(catalogueconstructor.signals_medians)\n",
    "print(catalogueconstructor.signals_mads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the main loop: signal preprocessing + peak detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_signalprocessor 0.8870328359998894 s\n",
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  nb_peak_by_segment: 648, 677\n",
      "  cluster_labels 0 [-11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.run_signalprocessor(duration=60.)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print('run_signalprocessor', t2-t1, 's')\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract some waveforms\n",
    "\n",
    "Take some waveforms in the signals *n_left/n_right* must be choosen arbitrary but lon enought.\n",
    "Better limits will be set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  nb_peak_by_segment: 648, 677\n",
      "  some_waveforms.shape: (1325, 65, 4) \n",
      "  cluster_labels 1 [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor.extract_some_waveforms(n_left=-25, n_right=40, mode='rand', nb_max=10000)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean waveforms\n",
    "\n",
    "Whis try to detect bad waveforms to not include them in features aand clustering.\n",
    "Strange waveforms are tag with -9 (alien)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  nb_peak_by_segment: 648, 677\n",
      "  some_waveforms.shape: (1325, 65, 4) \n",
      "  cluster_labels 1 [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor.clean_waveforms(alien_value_threshold=100.)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project to smaller space\n",
    "\n",
    "To reduce dimension of the waveforms (1323, 24, 4) we chosse global_pac method which is appropriate for tetrode.\n",
    "It consists of flatenning some_waveforms.shape (1323, 24, 4) to (1323, 24x4) and then apply a standard PCA on it with sklearn.\n",
    "\n",
    "Let's keep 5 component of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project 0.14626117900024838\n",
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  nb_peak_by_segment: 648, 677\n",
      "  some_waveforms.shape: (1325, 65, 4) \n",
      "  some_features.shape: (1325, 5)\n",
      "  cluster_labels 1 [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.extract_some_features(method='global_pca', n_components=5)\n",
    "t2 = time.perf_counter()\n",
    "print('project', t2-t1)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find clusters\n",
    "\n",
    "There are many option to cluster this features. here a simple one the well known kmeans method.\n",
    "\n",
    "Unfortunatly we need to choose the number of cluster. Too bad... Let's take 12.\n",
    "\n",
    "Later on we will be able to refine this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find_clusters 0.33993482700043387\n",
      "CatalogueConstructor\n",
      "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
      "  nb_peak_by_segment: 648, 677\n",
      "  some_waveforms.shape: (1325, 65, 4) \n",
      "  some_features.shape: (1325, 5)\n",
      "  cluster_labels 12 [0 1 2 ... 10 11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.find_clusters(method='kmeans', n_clusters=12)\n",
    "t2 = time.perf_counter()\n",
    "print('find_clusters', t2-t1)\n",
    "print(catalogueconstructor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open CatalogueWindow for visual check\n",
    "\n",
    "This open a CatalogueWindow, here we can check, split merge, trash, play as long as we are not happy.\n",
    "\n",
    "We happy, we can save the catalogue.\n",
    "\n",
    "Don't save nothing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.CatalogueWindow(catalogueconstructor)\n",
    "win.show()\n",
    "app.exec_()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a snappshot of CatalogueWindow\n",
    "\n",
    "<img src=\"../doc/img/snapshot_cataloguewindow.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty clean of catatalogue\n",
    "\n",
    "Here a quick and dirty clean of teh catalogue and them save it!!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_catalogue 0.044265984000048775\n"
     ]
    }
   ],
   "source": [
    "#order cluster by waveforms rms\n",
    "catalogueconstructor.order_clusters(by='waveforms_rms')\n",
    "\n",
    "#save the catalogue\n",
    "catalogueconstructor.make_catalogue_for_peeler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeler\n",
    "\n",
    "Create and run the Peeler.\n",
    "It should be pretty fast, here the computation take 1.32s for 28.8x2s of signal. This is a speed up of 43 over real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [00:00<00:00, 509.69it/s]\n",
      "100%|██████████| 421/421 [00:01<00:00, 363.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peeler.run 2.1866492569997718\n",
      "\n",
      "seg_num 0 nb_spikes 611\n",
      "seg_num 1 nb_spikes 646\n"
     ]
    }
   ],
   "source": [
    "initial_catalogue = dataio.load_catalogue(chan_grp=0)\n",
    "\n",
    "peeler = Peeler(dataio)\n",
    "peeler.change_params(catalogue=initial_catalogue)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "peeler.run()\n",
    "t2 = time.perf_counter()\n",
    "print('peeler.run', t2-t1)\n",
    "\n",
    "print()\n",
    "for seg_num in range(dataio.nb_segment):\n",
    "    spikes = dataio.get_spikes(seg_num)\n",
    "    print('seg_num', seg_num, 'nb_spikes', spikes.size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open PeelerWindow for visual checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.PeelerWindow(dataio=dataio, catalogue=initial_catalogue)\n",
    "win.show()\n",
    "app.exec_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a snappshot of PeelerWindow\n",
    "\n",
    "<img src=\"../doc/img/snapshot_peelerwindow.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
