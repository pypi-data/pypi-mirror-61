%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Group Lasso Documentation}
\date{Jan 23, 2020}
\release{1.1.1}
\author{Yngve Mardal Moe}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


This library provides efficient computation of sparse group lasso regularise
linear and logistic regression.


\chapter{What is group lasso?}
\label{\detokenize{index:what-is-group-lasso}}
It is often the case that we have a dataset where the covariates form natural
groups. These groups can represent biological function in gene expression
data or maybe sensor location in climate data. We then wish to find a sparse
subset of these covariate groups that describe the relationship in the data.
Let us look at an example to crystalise the usefulness of this further.

Say that we work as data scientists for a large Norwegian food supplier and
wish to make a prediction model for the amount of that will be sold based on
weather data. We have weather data from cities in Norway and need to know how
the fruit should be distributed across different warehouses. From each city,
we have information about temperature, precipitation, wind strength, wind
direction and how cloudy it is. Multiplying the number of cities with the
number of covariates per city, we get 1500 different covariates in total.
It is unlikely that we need all these covariates in our model, so we seek a
sparse set of these to do our predictions with.

Let us now assume that the weather data API that we use charge money by
the number of cities we query, but the amount of information we get per
city. We therefore wish to create a regression model that predicts fruit
demand based on a sparse set of city observations. One way to achieve such
sparsity is through the framework of group lasso regularisation %
\begin{footnote}[1]\sphinxAtStartFootnote
Yuan M, Lin Y. Model selection and estimation in regression with
grouped variables. Journal of the Royal Statistical Society: Series B
(Statistical Methodology). 2006 Feb;68(1):49-67.
%
\end{footnote}.


\chapter{What is sparse group lasso}
\label{\detokenize{index:what-is-sparse-group-lasso}}
The sparse group lasso regulariser %
\begin{footnote}[2]\sphinxAtStartFootnote
Simon, N., Friedman, J., Hastie, T., \& Tibshirani, R. (2013).
A sparse-group lasso. Journal of Computational and Graphical
Statistics, 22(2), 231-245.
%
\end{footnote} is an extension of the group lasso
regulariser that also promotes parameter-wise sparsity. It is the combination
of the group lasso penalty and the normal lasso penalty. If we consider the
example above, then the sparse group lasso penalty will yield a sparse set
of groups and also a sparse set of covariates in each selected group. An
example of where this is useful is if each city query has a set price that
increases based on the number of measurements we want from each city.


\chapter{A quick mathematical interlude}
\label{\detokenize{index:a-quick-mathematical-interlude}}
Let us now briefly describe the mathematical problem solved in group lasso
regularised machine learning problems. Originally, group lasso algorithm \sphinxfootnotemark[1]
was defined as regularised linear regression with the following loss function
\begin{equation*}
\begin{split}\text{arg} \min_{\mathbf{\beta}_g \in \mathbb{R^{d_g}}}
|| \sum_{g \in \mathcal{G}} \left[\mathbf{X}_g\mathbf{\beta}_g\right] - \mathbf{y} ||_2^2
+ \lambda_1 ||\mathbf{\beta}||_1
+ \lambda_2 \sum_{g \in \mathcal{G}} \sqrt{d_g}||\mathbf{\beta}_g||_2,\end{split}
\end{equation*}
where \(\mathbf{X}_g \in \mathbb{R}^{n \times d_g}\) is the data matrix
corresponding to the covariates in group \(g\), \(\mathbf{\beta}_g\)
is the regression coefficients corresponding to group \(g\),
\(\mathbf{y} \in \mathbf{R}^n\) is the regression target, \(n\) is the
number of measurements, \(d_g\) is the dimensionality of group \(g\),
\(\lambda_1\) is the parameter-wise regularisation penalty,
\(\lambda_2\) is the group-wise regularisation penalty and
\(\mathcal{G}\) is the set of all groups.

Notice, in the equation above, that the 2-norm is \sphinxstyleemphasis{not} squared. A consequence
of this is that the regulariser has a “kink” at zero, uninformative covariate
groups to have zero-valued regression coefficients. Later, it has been popular
to use this methodology to regularise other machine learning algorithms, such
as logistic regression. The “only” thing neccesary to do this is to exchange
the squared norm term, \(|| \sum_{g \in \mathcal{G}} \left[\mathbf{X}_g\mathbf{\beta}_g\right] - \mathbf{y} ||_2^2\),
with a general loss term, \(L(\mathbf{\beta}; \mathbf{X}, \mathbf{y})\),
where \(\mathbf{\beta}\) and \(\mathbf{X}\) is the concatenation
of all group coefficients and group data matrices, respectively.


\chapter{API design}
\label{\detokenize{index:api-design}}
The \sphinxcode{\sphinxupquote{group-lasso}} python library is modelled after the \sphinxcode{\sphinxupquote{scikit-learn}} API
and should be fully compliant with the \sphinxcode{\sphinxupquote{scikit-learn}} ecosystem.
Consequently, the \sphinxcode{\sphinxupquote{group-lasso}} library depends on \sphinxcode{\sphinxupquote{numpy}}, \sphinxcode{\sphinxupquote{scipy}}
and \sphinxcode{\sphinxupquote{scikit-learn}}.

Currently, the only supported algorithm is group-lasso regularised linear
and multiple regression, which is available in the \sphinxcode{\sphinxupquote{group\_lasso.GroupLasso}}
class. However, I am working on an experimental class with group lasso
regularised logistic regression, which is available in the
\sphinxcode{\sphinxupquote{group\_lasso.LogisticGroupLasso}} class. Currently, this class only supports
binary classification problems through a sigmoidal transformation, but
I am working on a multiple classification algorithm with the softmax
transformation.

All classes in this library is implemented as both \sphinxcode{\sphinxupquote{scikit-learn}}
transformers and their regressors or classifiers (dependent on their
use case). The reason for this is that to use lasso based models for
variable selection, the regularisation coefficient should be quite high,
resulting in sub-par performance on the actual task of interest. Therefore,
it is common to first use a lasso-like algorithm to select the relevant
features before using another another algorithm (say ridge regression)
for the task at hand. Therefore, the \sphinxcode{\sphinxupquote{transform}} method of
\sphinxcode{\sphinxupquote{group\_lasso.GroupLasso}} to remove the columns of the input dataset
corresponding to zero-valued coefficients.


\section{Installation guide}
\label{\detokenize{installation:installation-guide}}\label{\detokenize{installation::doc}}

\subsection{Dependencies}
\label{\detokenize{installation:dependencies}}
\sphinxcode{\sphinxupquote{group-lasso}} support Python 3.5+, Additionally, you will need \sphinxcode{\sphinxupquote{numpy}},
and \sphinxcode{\sphinxupquote{scikit-learn}} (which again requires \sphinxcode{\sphinxupquote{scipy}} and \sphinxcode{\sphinxupquote{joblib}}). These
packages should come pre-installed on any Anaconda installation, otherwise,
they can be installed using \sphinxcode{\sphinxupquote{pip}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{numpy}
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{scikit}\PYG{o}{\PYGZhy{}}\PYG{n}{learn}
\end{sphinxVerbatim}


\subsection{Installing group-lasso}
\label{\detokenize{installation:installing-group-lasso}}
\sphinxcode{\sphinxupquote{group-lasso}} is available through Pypi and can easily be installed with a
pip install:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{group}\PYG{o}{\PYGZhy{}}\PYG{n}{lasso}
\end{sphinxVerbatim}

I update the Pypi version regularly, however for the latest update, you should
clone from GitHub and install it directly, as so:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{yngvem}\PYG{o}{/}\PYG{n}{group}\PYG{o}{\PYGZhy{}}\PYG{n}{lasso}\PYG{o}{.}\PYG{n}{git}
\PYG{n}{cd} \PYG{n}{group}\PYG{o}{\PYGZhy{}}\PYG{n}{lasso}
\PYG{n}{python} \PYG{n}{setup}\PYG{o}{.}\PYG{n}{py}
\end{sphinxVerbatim}


\section{Examples}
\label{\detokenize{examples:examples}}\label{\detokenize{examples::doc}}

\subsection{Group lasso regression}
\label{\detokenize{examples:group-lasso-regression}}
The group lasso regulariser is implemented following the scikit-learn API,
making it easy to use for those familiar with the Python ML ecosystem.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{group\PYGZus{}lasso} \PYG{k+kn}{import} \PYG{n}{GroupLasso}

\PYG{c+c1}{\PYGZsh{} Dataset parameters}
\PYG{n}{num\PYGZus{}data\PYGZus{}points} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{num\PYGZus{}features} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{num\PYGZus{}groups} \PYG{o}{=} \PYG{l+m+mi}{25}
\PYG{k}{assert} \PYG{n}{num\PYGZus{}features} \PYG{o}{\PYGZpc{}} \PYG{n}{num\PYGZus{}groups} \PYG{o}{==} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} Generate data matrix}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{num\PYGZus{}data\PYGZus{}points}\PYG{p}{,} \PYG{n}{num\PYGZus{}features}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate coefficients and intercept}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{intercept} \PYG{o}{=} \PYG{l+m+mi}{2}

\PYG{c+c1}{\PYGZsh{} Generate groups and randomly set coefficients to zero}
\PYG{n}{groups} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{group}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{20} \PYG{k}{for} \PYG{n}{group} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{group} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}groups}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{w}\PYG{p}{[}\PYG{n}{groups} \PYG{o}{==} \PYG{n}{group}\PYG{p}{]} \PYG{o}{*}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.8}

\PYG{c+c1}{\PYGZsh{} Generate target vector:}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{X}\PYG{n+nd}{@w} \PYG{o}{+} \PYG{n}{intercept}
\PYG{n}{noise} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{n}{y}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n}{noise} \PYG{o}{/}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{noise}\PYG{p}{)}
\PYG{n}{noise} \PYG{o}{*}\PYG{o}{=} \PYG{l+m+mf}{0.3}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{+}\PYG{o}{=} \PYG{n}{noise}

\PYG{c+c1}{\PYGZsh{} Generate group lasso object and fit the model}
\PYG{n}{gl} \PYG{o}{=} \PYG{n}{GroupLasso}\PYG{p}{(}\PYG{n}{groups}\PYG{o}{=}\PYG{n}{groups}\PYG{p}{,} \PYG{n}{group\PYGZus{}reg}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{l1\PYGZus{}reg}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
\PYG{n}{gl}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{estimated\PYGZus{}w} \PYG{o}{=} \PYG{n}{gl}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}
\PYG{n}{estimated\PYGZus{}intercept} \PYG{o}{=} \PYG{n}{gl}\PYG{o}{.}\PYG{n}{intercept\PYGZus{}}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Evaluate the model}
\PYG{n}{coef\PYGZus{}correlation} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{corrcoef}\PYG{p}{(}\PYG{n}{w}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{estimated\PYGZus{}w}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{k}{print}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{True intercept: \PYGZob{}intercept:.2f\PYGZcb{}. Estimated intercept: \PYGZob{}estimated\PYGZus{}intercept:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}
        \PYG{n}{estimated\PYGZus{}intercept}\PYG{o}{=}\PYG{n}{estimated\PYGZus{}intercept}
    \PYG{p}{)}
\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Correlation between true and estimated coefficients: \PYGZob{}coef\PYGZus{}correlation:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}
        \PYG{n}{coef\PYGZus{}correlation}\PYG{o}{=}\PYG{n}{coef\PYGZus{}correlation}
     \PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
True intercept: 2.00. Estimated intercept: 1.53
Correlation between true and estimated coefficients: 0.98
\end{sphinxVerbatim}


\subsection{Group lasso as a transformer}
\label{\detokenize{examples:group-lasso-as-a-transformer}}
Group lasso regression can also be used as a transformer

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k+kn}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.pipeline} \PYG{k+kn}{import} \PYG{n}{Pipeline}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn.linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{Ridge}
\PYG{k+kn}{from} \PYG{n+nn}{group\PYGZus{}lasso} \PYG{k+kn}{import} \PYG{n}{GroupLasso}

\PYG{c+c1}{\PYGZsh{} Dataset parameters}
\PYG{n}{num\PYGZus{}data\PYGZus{}points} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{num\PYGZus{}features} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{num\PYGZus{}groups} \PYG{o}{=} \PYG{l+m+mi}{25}
\PYG{k}{assert} \PYG{n}{num\PYGZus{}features} \PYG{o}{\PYGZpc{}} \PYG{n}{num\PYGZus{}groups} \PYG{o}{==} \PYG{l+m+mi}{0}

\PYG{c+c1}{\PYGZsh{} Generate data matrix}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{p}{(}\PYG{n}{num\PYGZus{}data\PYGZus{}points}\PYG{p}{,} \PYG{n}{num\PYGZus{}features}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate coefficients and intercept}
\PYG{n}{w} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{intercept} \PYG{o}{=} \PYG{l+m+mi}{2}

\PYG{c+c1}{\PYGZsh{} Generate groups and randomly set coefficients to zero}
\PYG{n}{groups} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{group}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mi}{20} \PYG{k}{for} \PYG{n}{group} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{group} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}groups}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{w}\PYG{p}{[}\PYG{n}{groups} \PYG{o}{==} \PYG{n}{group}\PYG{p}{]} \PYG{o}{*}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.8}

\PYG{c+c1}{\PYGZsh{} Generate target vector:}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{X}\PYG{n+nd}{@w} \PYG{o}{+} \PYG{n}{intercept}
\PYG{n}{noise} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{standard\PYGZus{}normal}\PYG{p}{(}\PYG{n}{y}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n}{noise} \PYG{o}{/}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{noise}\PYG{p}{)}
\PYG{n}{noise} \PYG{o}{*}\PYG{o}{=} \PYG{l+m+mf}{0.3}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{y}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{+}\PYG{o}{=} \PYG{n}{noise}

\PYG{c+c1}{\PYGZsh{} Generate group lasso object and fit the model}
\PYG{c+c1}{\PYGZsh{} We use an artificially high regularisation coefficient since}
\PYG{c+c1}{\PYGZsh{}  we want to use group lasso as a variable selection algorithm.}
\PYG{n}{gl} \PYG{o}{=} \PYG{n}{GroupLasso}\PYG{p}{(}\PYG{n}{groups}\PYG{o}{=}\PYG{n}{groups}\PYG{p}{,} \PYG{n}{group\PYGZus{}reg}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{l1\PYGZus{}reg}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{)}
\PYG{n}{gl}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{new\PYGZus{}X} \PYG{o}{=} \PYG{n}{gl}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Evaluate the model}
\PYG{n}{predicted\PYGZus{}y} \PYG{o}{=} \PYG{n}{gl}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{R\PYGZus{}squared} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The rows with zero\PYGZhy{}valued coefficients have now been removed from the dataset.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The new shape is:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{new\PYGZus{}X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The R\PYGZca{}2 statistic for the group lasso model is: \PYGZob{}R\PYGZus{}squared:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{R\PYGZus{}squared}\PYG{o}{=}\PYG{n}{R\PYGZus{}squared}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{This is very low since the regularisation is so high.}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Use group lasso in a scikit\PYGZhy{}learn pipeline}
\PYG{n}{pipe} \PYG{o}{=} \PYG{n}{Pipeline}\PYG{p}{(}
    \PYG{n}{memory}\PYG{o}{=}\PYG{n+nb+bp}{None}\PYG{p}{,}
    \PYG{n}{steps}\PYG{o}{=}\PYG{p}{[}
        \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{variable\PYGZus{}selection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{GroupLasso}\PYG{p}{(}\PYG{n}{groups}\PYG{o}{=}\PYG{n}{groups}\PYG{p}{,} \PYG{n}{reg}\PYG{o}{=}\PYG{o}{.}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
        \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{regressor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{Ridge}\PYG{p}{(}\PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
    \PYG{p}{]}
\PYG{p}{)}
\PYG{n}{pipe}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{predicted\PYGZus{}y} \PYG{o}{=} \PYG{n}{pipe}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{R\PYGZus{}squared} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{predicted\PYGZus{}y}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{y}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The R\PYGZca{}2 statistic for the pipeline is: \PYGZob{}R\PYGZus{}squared:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{R\PYGZus{}squared}\PYG{o}{=}\PYG{n}{R\PYGZus{}squared}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
The rows with zero\PYGZhy{}valued coefficients have now been removed from the dataset.
The new shape is: (10000, 280)
The R\PYGZca{}2 statistic for the group lasso model is: 0.17
This is very low since the regularisation is so high.
The R\PYGZca{}2 statistic for the pipeline is: 0.72
\end{sphinxVerbatim}


\section{Mathematical background}
\label{\detokenize{maths:mathematical-background}}\label{\detokenize{maths::doc}}

\subsection{Quick overview}
\label{\detokenize{maths:quick-overview}}
Let us recap the definition of a sparse group lasso regularised machine
learning algorithm. Consdier the unregularised loss function
\(L(\mathbf{\beta}; \mathbf{X}, \mathbf{y})\), where
\(\mathbf{\beta}\) is the model coefficients, \(\mathbf{X}\) is the
data matrix and \(\mathbf{y}\) is the target vector (or matrix in the
case of multiple regression/classification algorithms). Furthermore, we
assume that
\(\mathbf{\beta} = \left[\mathbf{\beta}_1^T, ..., \mathbf{\beta}_G^T\right]^T\)
and that \(\mathbf{X} = \left[\mathbf{X}_1^T, ..., \mathbf{X}_G^T\right]^T\),
where \(\mathbf{\beta}_g\) and \(\mathbf{X}_g\) is the coefficients
and data matrices corresponding to covariate group \(g\). In this case, we
define the group lasso regularised loss function as
\begin{equation*}
\begin{split}L(\mathbf{\beta}; \mathbf{X}, \mathbf{y})
 + \lambda_1 ||\mathbf{\beta}||_1
 + \lambda_2 \sum_{g \in \mathcal{G}} \sqrt{d_g} ||\mathbf{\beta}||_2\end{split}
\end{equation*}
where \(\lambda_1\) is the parameter-wise regularisation penalty,
\(\lambda_2\) is the group-wise regularisation penalty,
\(\mathbf{\beta}_g \in \mathbf{d_g}\) and
\(\mathcal{G}\) is the set of all groups.

The above regularisation penalty is nice in the sense that it promotes that a
sparse set of groups are chosen for the regularisation coefficients %
\begin{footnote}[1]\sphinxAtStartFootnote
Yuan M, Lin Y. Model selection and estimation in regression with
grouped variables. Journal of the Royal Statistical Society: Series B
(Statistical Methodology). 2006 Feb;68(1):49-67.
%
\end{footnote}.
However, the non-continuous derivative makes the optimisation procedure much
more complicated than with say a Ridge penalty (i.e. squared 2-norm penalty).
One common algorithm used to solve this optimisation problem is
\sphinxstyleemphasis{group coordinate descent}, in which the optimisation problem is solved for
each group separately, in an alternating fashion. However, I decided to use
the fast iterative soft thresholding (FISTA) algorithm %
\begin{footnote}[2]\sphinxAtStartFootnote
Beck A, Teboulle M. A fast iterative shrinkage-thresholding algorithm
for linear inverse problems. SIAM journal on imaging sciences.
2009 Mar 4;2(1):183-202.
%
\end{footnote} with the
gradient-based restarting scheme given in %
\begin{footnote}[3]\sphinxAtStartFootnote
O’Donoghue B, Candes E. Adaptive restart for accelerated gradient
schemes. Foundations of computational mathematics.
2015 Jun 1;15(3):715-32.
%
\end{footnote}. This is regarded as one of the
best algorithms to solve optimisation problems on the form
\begin{equation*}
\begin{split}\text{arg} \min_{\mathbf{\beta}} L(\mathbf{\beta}) + R(\mathbf{\beta}),\end{split}
\end{equation*}
where \(L\) is a convex, differentiable function with Lipschitz continuous
gradient and \(R\) is a convex lower semicontinouous function.


\subsection{Details on FISTA}
\label{\detokenize{maths:details-on-fista}}
There are three essential parts of having an efficient implementation of the
FISTA algorithm. First and foremost, we need an efficient way to compute the
gradient of the loss function. Next, and just as important, we need to be able
to compute the \sphinxstyleemphasis{proximal map} of the regulariser efficiently. That is, we need
to know how to compute
\begin{equation*}
\begin{split}prox(\mathbf{\beta}) = \text{arg} \min_{\hat{\mathbf{\beta}}}
R(\hat{\mathbf{\beta}}) + \frac{1}{2}||\hat{\mathbf{\beta}} - \mathbf{\beta}||_2^2\end{split}
\end{equation*}
efficiently. To compute the proximal map for the sparse group lasso regulariser,
we use the following identity from %
\begin{footnote}[4]\sphinxAtStartFootnote
Yuan L, Liu J, Ye J. (2011), Efficient methods for overlapping group
lasso. Advances in Neural Information Processing Systems (pp. 352-360).
%
\end{footnote}:
\begin{equation*}
\begin{split}prox_{\lambda_1 ||\mathbf{\cdot}||_1 + \lambda_2 \sum_g w_g ||\mathbf{\cdot}||}(\mathbf{\beta})
= prox_{\lambda_2 \sum_g w_g ||\mathbf{\cdot}||}(prox_{\lambda_1 ||\mathbf{\cdot}||_1}(\mathbf{\beta}),\end{split}
\end{equation*}
where \(prox_{\lambda_1 ||\mathbf{\cdot}||_1 + \lambda_2 \sum_g w_g ||\mathbf{\cdot}||}\)
is the proximal map for the sparse group lasso regulariser,
\(prox_{\lambda_2 \sum_g w_g ||\mathbf{\cdot}||}\) is the proximal map
for the group lasso regulariser and
\(prox_{\lambda_1 ||\mathbf{\cdot}||_1\) is the proximal map for the
lasso regulariser. For more information on the proximal map, see %
\begin{footnote}[5]\sphinxAtStartFootnote
Parikh, N., \& Boyd, S. (2014). Proximal algorithms. Foundations and
Trends in Optimization, 1(3), 127-239.
%
\end{footnote} or %
\begin{footnote}[6]\sphinxAtStartFootnote
Beck, A. (2017). First-order methods in optimization (Vol. 25). SIAM.
%
\end{footnote}.
Finally, we need a Lipschitz bound for the gradient of the loss function, since
this is used to compute the step-length of the optimisation procedure. Luckily,
this can also be estimated using a line-search.

Unfortunately, the FISTA algorithm is not stable in the mini-batch case, making
it inefficient for extremely large datasets. However, in my experiments, I have
found that it still recovers the correct sparsity patterns in the data when used
in a mini-batch fashion for the group lasso problem. At least so long as the
mini-batches are relatively large.


\subsection{Computing the Lipschitz coefficients}
\label{\detokenize{maths:computing-the-lipschitz-coefficients}}
The Lipschitz coefficient of the gradient to the sum-of-squares loss is given
by \(\sigma_1^2\), where \(\sigma_1\) is the largest singular value
of the data matrix.

I have not found a published expression for the Lipschitz coefficient of the
sigmoid cross-entropy loss. Therefore, I derived the following bound:
\begin{equation*}
\begin{split}L = \sqrt{12} ||\mathbf{X}||_F,\end{split}
\end{equation*}
where \(||\mathbf{\cdot}||_F = \sqrt{\sum_{i, j} \mathbf{X}_{i, j}^2}\) is
the Frobenius norm. The next step to get group lasso regularised logistic
regression is deriving the Lipschitz bound for the gradient of the softmax
cross-entropy loss.


\subsection{References}
\label{\detokenize{maths:references}}

\section{API Reference}
\label{\detokenize{api_reference:api-reference}}\label{\detokenize{api_reference::doc}}\index{GroupLasso (class in group\_lasso)@\spxentry{GroupLasso}\spxextra{class in group\_lasso}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{group\_lasso.}}\sphinxbfcode{\sphinxupquote{GroupLasso}}}{\emph{groups=None}, \emph{group\_reg=0.05}, \emph{l1\_reg=0.05}, \emph{n\_iter=100}, \emph{tol=1e-05}, \emph{subsampling\_scheme=None}, \emph{fit\_intercept=True}, \emph{frobenius\_lipschitz=False}, \emph{random\_state=None}, \emph{warm\_start=False}}{}
Sparse group lasso regularised least squares linear regression.

This class implements the Sparse Group Lasso {[}1{]} regularisation for
linear regression with the mean squared penalty.

This class is implemented as both a regressor and a transformation.
If the \sphinxcode{\sphinxupquote{transform}} method is called, then the columns of the input
that correspond to zero-valued regression coefficients are dropped.

The loss is optimised using the FISTA algorithm proposed in {[}2{]} with the
generalised gradient-based restarting scheme proposed in {[}3{]}. This
algorithm is not as accurate as a few other optimisation algorithms,
but it is extremely efficient and does recover the sparsity patterns.
We therefore reccomend that this class is used as a transformer to select
the viable features and that the output is fed into another regression
algorithm, such as RidgeRegression in scikit-learn.
\subsubsection*{References}

{[}1{]} Simon, N., Friedman, J., Hastie, T., \& Tibshirani, R. (2013).
A sparse-group lasso. Journal of Computational and Graphical
Statistics, 22(2), 231-245.

{[}2{]} Beck A, Teboulle M. (2009). A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM journal on imaging
sciences. 2009 Mar 4;2(1):183-202.

{[}3{]} O’Donoghue B, Candes E. (2015) Adaptive restart for accelerated
gradient schemes. Foundations of computational mathematics.
Jun 1;15(3):715-32
\index{fit() (group\_lasso.GroupLasso method)@\spxentry{fit()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group lasso regularised linear regression model.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data matrix

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Target vector or matrix

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{lipschitz}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{None}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{default=None}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A Lipshitz bound for the mean squared loss with the given
data and target matrices. If None, this is estimated.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit\_transform() (group\_lasso.GroupLasso method)@\spxentry{fit\_transform()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.fit_transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit\_transform}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group lasso model to X and y and remove unused columns from X

\end{fulllineitems}

\index{get\_params() (group\_lasso.GroupLasso method)@\spxentry{get\_params()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.get_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_params}}}{\emph{deep=True}}{}
Get parameters for this estimator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{deep}} (\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If True, will return the parameters for this estimator and
contained subobjects that are estimators.

\item[{Returns}] \leavevmode
\sphinxstylestrong{params} \textendash{} Parameter names mapped to their values.

\item[{Return type}] \leavevmode
mapping of string to any

\end{description}\end{quote}

\end{fulllineitems}

\index{loss() (group\_lasso.GroupLasso method)@\spxentry{loss()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{X}, \emph{y}}{}
The group-lasso regularised loss with the current coefficients
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data matrix, \sphinxcode{\sphinxupquote{X.shape == (num\_datapoints, num\_features)}}

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Target vector/matrix, \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints, num\_targets)}},
or \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints,)}}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (group\_lasso.GroupLasso method)@\spxentry{predict()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{X}}{}
Predict using the linear model.

\end{fulllineitems}

\index{score() (group\_lasso.GroupLasso method)@\spxentry{score()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.score}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{score}}}{\emph{X}, \emph{y}, \emph{sample\_weight=None}}{}
Returns the coefficient of determination R\textasciicircum{}2 of the prediction.

The coefficient R\textasciicircum{}2 is defined as (1 - u/v), where u is the residual
sum of squares ((y\_true - y\_pred) ** 2).sum() and v is the total
sum of squares ((y\_true - y\_true.mean()) ** 2).sum().
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R\textasciicircum{}2 score of 0.0.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_features}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Test samples. For some estimators this may be a
precomputed kernel matrix instead, shape = (n\_samples,
n\_samples\_fitted{]}, where n\_samples\_fitted is the number of
samples used in the fitting for the estimator.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{) or }}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_outputs}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} True values for X.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{sample\_weight}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample weights.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxstylestrong{score} \textendash{} R\textasciicircum{}2 of self.predict(X) wrt. y.

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}
\subsubsection*{Notes}

The R2 score used when calling \sphinxcode{\sphinxupquote{score}} on a regressor will use
\sphinxcode{\sphinxupquote{multioutput='uniform\_average'}} from version 0.23 to keep consistent
with \sphinxtitleref{metrics.r2\_score}. This will influence the \sphinxcode{\sphinxupquote{score}} method of
all the multioutput regressors (except for
\sphinxtitleref{multioutput.MultiOutputRegressor}). To specify the default value
manually and avoid the warning, please either call \sphinxtitleref{metrics.r2\_score}
directly or make a custom scorer with \sphinxtitleref{metrics.make\_scorer} (the
built-in scorer \sphinxcode{\sphinxupquote{'r2'}} uses \sphinxcode{\sphinxupquote{multioutput='uniform\_average'}}).

\end{fulllineitems}

\index{set\_params() (group\_lasso.GroupLasso method)@\spxentry{set\_params()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.set_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_params}}}{\emph{**params}}{}
Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
\sphinxcode{\sphinxupquote{\textless{}component\textgreater{}\_\_\textless{}parameter\textgreater{}}} so that it’s possible to update each
component of a nested object.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
self

\end{description}\end{quote}

\end{fulllineitems}

\index{sparsity\_mask() (group\_lasso.GroupLasso property)@\spxentry{sparsity\_mask()}\spxextra{group\_lasso.GroupLasso property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.sparsity_mask}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{sparsity\_mask}}}
A boolean mask indicating whether features are used in prediction.

\end{fulllineitems}

\index{subsample() (group\_lasso.GroupLasso method)@\spxentry{subsample()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.subsample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{subsample}}}{\emph{*args}}{}
Subsample the input using this class’s subsampling scheme.

\end{fulllineitems}

\index{transform() (group\_lasso.GroupLasso method)@\spxentry{transform()}\spxextra{group\_lasso.GroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.GroupLasso.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{X}}{}
Remove columns corresponding to zero-valued coefficients.

\end{fulllineitems}


\end{fulllineitems}

\index{LogisticGroupLasso (class in group\_lasso)@\spxentry{LogisticGroupLasso}\spxextra{class in group\_lasso}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{group\_lasso.}}\sphinxbfcode{\sphinxupquote{LogisticGroupLasso}}}{\emph{groups}, \emph{group\_reg=0.05}, \emph{l1\_reg=0.05}, \emph{n\_iter=100}, \emph{tol=1e-05}, \emph{subsampling\_scheme=None}, \emph{fit\_intercept=True}, \emph{random\_state=None}, \emph{warm\_start=False}, \emph{old\_regularisation=False}, \emph{supress\_warning=False}}{}
Sparse group lasso regularised single-class logistic regression.

This class implements the Sparse Group Lasso {[}1{]} regularisation for
logistic regression with a cross entropy penalty.

This class is implemented as both a regressor and a transformation.
If the \sphinxcode{\sphinxupquote{transform}} method is called, then the columns of the input
that correspond to zero-valued regression coefficients are dropped.

The loss is optimised using the FISTA algorithm proposed in {[}2{]} with the
generalised gradient-based restarting scheme proposed in {[}3{]}. This
algorithm is not as accurate as a few other optimisation algorithms,
but it is extremely efficient and does recover the sparsity patterns.
We therefore reccomend that this class is used as a transformer to select
the viable features and that the output is fed into another classification
algorithm, such as LogisticRegression in scikit-learn.
\subsubsection*{References}

{[}1{]} Simon, N., Friedman, J., Hastie, T., \& Tibshirani, R. (2013).
A sparse-group lasso. Journal of Computational and Graphical
Statistics, 22(2), 231-245.

{[}2{]} Beck A, Teboulle M. (2009). A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM journal on imaging
sciences. 2009 Mar 4;2(1):183-202.

{[}3{]} O’Donoghue B, Candes E. (2015) Adaptive restart for accelerated
gradient schemes. Foundations of computational mathematics.
Jun 1;15(3):715-32.
\index{fit() (group\_lasso.LogisticGroupLasso method)@\spxentry{fit()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group-lasso regularised linear model.

\end{fulllineitems}

\index{fit\_transform() (group\_lasso.LogisticGroupLasso method)@\spxentry{fit\_transform()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.fit_transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit\_transform}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group lasso model to X and y and remove unused columns from X

\end{fulllineitems}

\index{get\_params() (group\_lasso.LogisticGroupLasso method)@\spxentry{get\_params()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.get_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_params}}}{\emph{deep=True}}{}
Get parameters for this estimator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{deep}} (\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If True, will return the parameters for this estimator and
contained subobjects that are estimators.

\item[{Returns}] \leavevmode
\sphinxstylestrong{params} \textendash{} Parameter names mapped to their values.

\item[{Return type}] \leavevmode
mapping of string to any

\end{description}\end{quote}

\end{fulllineitems}

\index{loss() (group\_lasso.LogisticGroupLasso method)@\spxentry{loss()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{X}, \emph{y}}{}
The group-lasso regularised loss with the current coefficients
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data matrix, \sphinxcode{\sphinxupquote{X.shape == (num\_datapoints, num\_features)}}

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Target vector/matrix, \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints, num\_targets)}},
or \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints,)}}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (group\_lasso.LogisticGroupLasso method)@\spxentry{predict()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{X}}{}
Predict using the linear model.

\end{fulllineitems}

\index{score() (group\_lasso.LogisticGroupLasso method)@\spxentry{score()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.score}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{score}}}{\emph{X}, \emph{y}, \emph{sample\_weight=None}}{}
Returns the mean accuracy on the given test data and labels.

In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_features}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Test samples.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{) or }}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_outputs}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} True labels for X.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{sample\_weight}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample weights.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxstylestrong{score} \textendash{} Mean accuracy of self.predict(X) wrt. y.

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_params() (group\_lasso.LogisticGroupLasso method)@\spxentry{set\_params()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.set_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_params}}}{\emph{**params}}{}
Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
\sphinxcode{\sphinxupquote{\textless{}component\textgreater{}\_\_\textless{}parameter\textgreater{}}} so that it’s possible to update each
component of a nested object.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
self

\end{description}\end{quote}

\end{fulllineitems}

\index{sparsity\_mask() (group\_lasso.LogisticGroupLasso property)@\spxentry{sparsity\_mask()}\spxextra{group\_lasso.LogisticGroupLasso property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.sparsity_mask}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{sparsity\_mask}}}
A boolean mask indicating whether features are used in prediction.

\end{fulllineitems}

\index{subsample() (group\_lasso.LogisticGroupLasso method)@\spxentry{subsample()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.subsample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{subsample}}}{\emph{*args}}{}
Subsample the input using this class’s subsampling scheme.

\end{fulllineitems}

\index{transform() (group\_lasso.LogisticGroupLasso method)@\spxentry{transform()}\spxextra{group\_lasso.LogisticGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.LogisticGroupLasso.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{X}}{}
Remove columns corresponding to zero-valued coefficients.

\end{fulllineitems}


\end{fulllineitems}

\index{MultinomialGroupLasso (class in group\_lasso)@\spxentry{MultinomialGroupLasso}\spxextra{class in group\_lasso}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{group\_lasso.}}\sphinxbfcode{\sphinxupquote{MultinomialGroupLasso}}}{\emph{groups}, \emph{group\_reg=0.05}, \emph{l1\_reg=0.05}, \emph{n\_iter=100}, \emph{tol=1e-05}, \emph{subsampling\_scheme=None}, \emph{fit\_intercept=True}, \emph{random\_state=None}, \emph{warm\_start=False}}{}
Sparse group lasso regularised multi-class logistic regression.

This class implements the Sparse Group Lasso {[}1{]} regularisation for
multinomial regression (also known as multi-class logistic regression)
with a cross entropy penalty.

This class is implemented as both a regressor and a transformation.
If the \sphinxcode{\sphinxupquote{transform}} method is called, then the columns of the input
that correspond to zero-valued regression coefficients are dropped.

The loss is optimised using the FISTA algorithm proposed in {[}2{]} with the
generalised gradient-based restarting scheme proposed in {[}3{]}. This
algorithm is not as accurate as a few other optimisation algorithms,
but it is extremely efficient and does recover the sparsity patterns.
We therefore reccomend that this class is used as a transformer to select
the viable features and that the output is fed into another classification
algorithm, such as LogisticRegression in scikit-learn.
\subsubsection*{References}

{[}1{]} Simon, N., Friedman, J., Hastie, T., \& Tibshirani, R. (2013).
A sparse-group lasso. Journal of Computational and Graphical
Statistics, 22(2), 231-245.

{[}2{]} Beck A, Teboulle M. (2009). A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM journal on imaging
sciences. 2009 Mar 4;2(1):183-202.

{[}3{]} O’Donoghue B, Candes E. (2015) Adaptive restart for accelerated
gradient schemes. Foundations of computational mathematics.
Jun 1;15(3):715-32
\index{fit() (group\_lasso.MultinomialGroupLasso method)@\spxentry{fit()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.fit}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group-lasso regularised linear model.

\end{fulllineitems}

\index{fit\_transform() (group\_lasso.MultinomialGroupLasso method)@\spxentry{fit\_transform()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.fit_transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{fit\_transform}}}{\emph{X}, \emph{y}, \emph{lipschitz=None}}{}
Fit a group lasso model to X and y and remove unused columns from X

\end{fulllineitems}

\index{get\_params() (group\_lasso.MultinomialGroupLasso method)@\spxentry{get\_params()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.get_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_params}}}{\emph{deep=True}}{}
Get parameters for this estimator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{deep}} (\sphinxstyleliteralemphasis{\sphinxupquote{boolean}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If True, will return the parameters for this estimator and
contained subobjects that are estimators.

\item[{Returns}] \leavevmode
\sphinxstylestrong{params} \textendash{} Parameter names mapped to their values.

\item[{Return type}] \leavevmode
mapping of string to any

\end{description}\end{quote}

\end{fulllineitems}

\index{loss() (group\_lasso.MultinomialGroupLasso method)@\spxentry{loss()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.loss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{X}, \emph{y}}{}
The group-lasso regularised loss with the current coefficients
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data matrix, \sphinxcode{\sphinxupquote{X.shape == (num\_datapoints, num\_features)}}

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Target vector/matrix, \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints, num\_targets)}},
or \sphinxcode{\sphinxupquote{y.shape == (num\_datapoints,)}}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (group\_lasso.MultinomialGroupLasso method)@\spxentry{predict()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{X}}{}
Predict using the linear model.

\end{fulllineitems}

\index{score() (group\_lasso.MultinomialGroupLasso method)@\spxentry{score()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.score}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{score}}}{\emph{X}, \emph{y}, \emph{sample\_weight=None}}{}
Returns the mean accuracy on the given test data and labels.

In multi-label classification, this is the subset accuracy
which is a harsh metric since you require for each sample that
each label set be correctly predicted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{X}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_features}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Test samples.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ (}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{) or }}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{n\_outputs}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} True labels for X.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{sample\_weight}} (\sphinxstyleliteralemphasis{\sphinxupquote{array-like}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{shape =}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{n\_samples}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample weights.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxstylestrong{score} \textendash{} Mean accuracy of self.predict(X) wrt. y.

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_params() (group\_lasso.MultinomialGroupLasso method)@\spxentry{set\_params()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.set_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_params}}}{\emph{**params}}{}
Set the parameters of this estimator.

The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
\sphinxcode{\sphinxupquote{\textless{}component\textgreater{}\_\_\textless{}parameter\textgreater{}}} so that it’s possible to update each
component of a nested object.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode


\item[{Return type}] \leavevmode
self

\end{description}\end{quote}

\end{fulllineitems}

\index{sparsity\_mask() (group\_lasso.MultinomialGroupLasso property)@\spxentry{sparsity\_mask()}\spxextra{group\_lasso.MultinomialGroupLasso property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.sparsity_mask}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{sparsity\_mask}}}
A boolean mask indicating whether features are used in prediction.

\end{fulllineitems}

\index{subsample() (group\_lasso.MultinomialGroupLasso method)@\spxentry{subsample()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.subsample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{subsample}}}{\emph{*args}}{}
Subsample the input using this class’s subsampling scheme.

\end{fulllineitems}

\index{transform() (group\_lasso.MultinomialGroupLasso method)@\spxentry{transform()}\spxextra{group\_lasso.MultinomialGroupLasso method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{api_reference:group_lasso.MultinomialGroupLasso.transform}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform}}}{\emph{X}}{}
Remove columns corresponding to zero-valued coefficients.

\end{fulllineitems}


\end{fulllineitems}



\chapter{References}
\label{\detokenize{index:references}}


\renewcommand{\indexname}{Index}
\printindex
\end{document}