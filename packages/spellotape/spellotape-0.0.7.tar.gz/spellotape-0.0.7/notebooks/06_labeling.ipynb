{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling\n",
    "\n",
    "This provides utilities for analyzing labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def labeler_agreement(df_full, mode='gold', label_col='new_y'):\n",
    "    \"\"\"Compute estimate of inter-labeler agreement for a binary classification \n",
    "    task from a Figure 8 full labeling report. We compute agreement scores for\n",
    "    each example, then average these scores over all examples to obtain our \n",
    "    estimate of overall agreement. Remember that due to quality control \n",
    "    measures, only labelers deemed \"reliable\" are included - if we were to \n",
    "    open the task to any labeler, performance would drop.\n",
    "\n",
    "    Note that this is one of several ways we could estimate human level \n",
    "    performance, each with its own pros and cons: \n",
    "    \n",
    "    - Take the aggregated report and compute a simple mean of confidence \n",
    "    scores for all examples.\n",
    "    - Take the full report and compute a simple mean of trust scores \n",
    "    (gold set accuracy) for all labelers.\n",
    "    - Take the full report and compute average agreement scores for each \n",
    "    example. (This is the method used here.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_full: pd.DataFrame\n",
    "        Full report from Figure 8, where each row contains a single judgment.\n",
    "    mode: str\n",
    "        One of ('gold', 'all', 'regular'). \"gold\" will compute the estimate \n",
    "        only using labels for the gold standard set (i.e. the test questions). \n",
    "        \"all\" will use the whole set for computation. \"regular\" will use only\n",
    "        non-test questions.\n",
    "    label_col: str\n",
    "        Name of column containing labels to assess.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: Value between 0 and 1 measuring the percent of labelers who\n",
    "        selected the majority class, averaged over all examples in the job.\n",
    "    \"\"\"\n",
    "    if mode == 'gold':\n",
    "        df = df_full[df_full._golden]\n",
    "    elif mode == 'regular':\n",
    "        df = df_full[~df_full._golden]\n",
    "    elif mode == 'all':\n",
    "        df = df_full\n",
    "        \n",
    "    def agreement(seq):\n",
    "        \"\"\"Binary inter-labeler agreement score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq: list, np.array, pd.Series\n",
    "            A sequence of binary labels for a single example.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float: Value between .5 and 1 measuring inter-labeler agreement.\n",
    "        \"\"\"\n",
    "        avg = np.mean(seq)\n",
    "        return max(avg, 1-avg)\n",
    "    \n",
    "    return df.groupby('_unit_id')[label_col].apply(agreement).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to compute `labeler_agreement` as a means of estimating human-level performance at a task. This provides one way to do that. We take a full report where each row contains a single labeler's opinion and compute agreement by sample id. Possible values range between 0.5 and 1, with 1 indicating perfect agreement and 0.5 indicating no consensus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
