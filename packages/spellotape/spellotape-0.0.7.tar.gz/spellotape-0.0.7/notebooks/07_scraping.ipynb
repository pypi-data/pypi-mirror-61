{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import praw\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from accio.s3tool import S3tool\n",
    "from spellotape.utils import IMG_EXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "This module provides tools for webscraping. Currently, it focuses on collecting images from Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fetch_img_from_url(url, out_path, verbose=False):\n",
    "    \"\"\"Given a URL, fetch an image and download it to the specified path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        Location of image online.\n",
    "    out_path: str\n",
    "        Path to download the image to.\n",
    "    verbose: bool\n",
    "        If True, prints a message alerting the user when the image could not be\n",
    "        retrieved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool: Specifies whether image was successfully retrieved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=10) as r:\n",
    "            if r.status_code != 200:\n",
    "                if verbose: print(f'STATUS CODE ERROR: {url}')\n",
    "                return False\n",
    "\n",
    "            # Write bytes to file chunk by chunk.\n",
    "            with open(out_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(256):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "    # Any time url cannot be accessed, don't care about exact error.\n",
    "    except Exception as e:\n",
    "        if verbose: print(e)\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fetch_img_from_url` fetches and downloads an image. It can handle fairly large image since it streams bytes chunk by chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_praw_credentials(bucket='gg-datascience', \n",
    "                          s3_path='hmamin/praw_creds.csv'):\n",
    "    \"\"\"Load credentials for dune script owned by hdmamin user on reddit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, str]: Keyword arguments that can be passed directly to \n",
    "        praw.Reddit to instantiate a new instance.\n",
    "    \"\"\"\n",
    "    tmp_dir = '/tmp'\n",
    "    tmp_file = os.path.join(tmp_dir, 'praw_creds.csv')\n",
    "    s3 = S3tool()\n",
    "    s3.download_file(bucket, s3_path, tmp_dir, maintain_s3_partitions=False)\n",
    "    \n",
    "    # Read credentials into dict and delete temporary file.\n",
    "    with open(tmp_file, 'r') as f:\n",
    "        cols, vals = [line.strip().split(',') for line in f]\n",
    "    os.remove(tmp_file)\n",
    "    return dict(zip(cols, vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_praw_credentials` loads a dictionary of credentials from S3 which can be used for user authorization in PRAW (Python Reddit API Wrapper). It can be passed directly to Praw's `Reddit` class as \\**kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def is_imgur_page(url):\n",
    "    \"\"\"Check if a URL is a link to an imgur html page. Use as validation before \n",
    "    trying to fetch an image directly.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    url = url.lower()\n",
    "    return 'imgur' in url and not any(ext in url.split('.')[-1] \n",
    "                                      for ext in IMG_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to determine whether a url is a link to an Imgur page containing additional html rather than a direct link to the image. This is often the case for image posts when scraping subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fetch_imgur_url(url):\n",
    "    \"\"\"Fetch and extract an image url from an imgur page. Returns None if an\n",
    "    image is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    \n",
    "    # If url throws error, see if it is one of the links formatted like this:\n",
    "    # ex: http://[Imgur](http://i.imgur.com/1gmLlya)\n",
    "    # If so, recursive call on part in parens. Otherwise, return None.\n",
    "    except:\n",
    "        parens_links = re.findall(r'\\((.+)\\)', url)\n",
    "        if parens_links:\n",
    "            return fetch_imgur_url(parens_links[0])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    if r.status_code != 200: return\n",
    "    img_links = BeautifulSoup(r.content, features='lxml')\\\n",
    "        .find_all('link', rel='image_src')\n",
    "    try:\n",
    "        return img_links[0]['href']\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another helper function when the url in question is determined to be an Imgur url (see above). This extracts a direct url to the image to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def scrape_subreddit_images_1k(sub_name, out_dir, n=None, verbose=True, \n",
    "                               print_freq=50):\n",
    "    \"\"\"Scrape subreddit using PRAW. Limited to 1,000 posts per call, and \n",
    "    haven't found way to use this iteratively to get all posts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sub_name: str\n",
    "        Name of subreddit.\n",
    "    out_dir: str\n",
    "        Name of output image directory.\n",
    "    n: int\n",
    "        Max number of posts to scrape. If None, scrapes as many as the\n",
    "        subreddit has (or until Praw limit is reached).\n",
    "    verbose: bool\n",
    "\n",
    "    print_freq: n\n",
    "        Frequency with which to print message with number of posts scraped.\n",
    "        Makes monitoring easier.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame: Two columns consisting of image urls and post urls.\n",
    "    \"\"\"\n",
    "    img_dir = os.path.join(out_dir, 'test', sub_name)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    creds = load_praw_credentials()\n",
    "    reddit = praw.Reddit(**creds)\n",
    "    sub = reddit.subreddit(sub_name)\n",
    "    \n",
    "    # Scrape top level image from each post.\n",
    "    posts = []\n",
    "    for i, post in enumerate(sub.top(limit=n), 1):\n",
    "        if i % print_freq == 0: print(f'Scraping post {i}...')\n",
    "        url = post.url\n",
    "        if is_imgur_page(url): url = fetch_imgur_url(url)\n",
    "        if not url: continue\n",
    "            \n",
    "        # Check image is fetched, then check file is not empty/corrupt.\n",
    "        fname = re.sub('\\?\\d*$', '', url.strip('/')).split('/')[-1]\n",
    "        img_path = os.path.join(img_dir, fname)\n",
    "        if fetch_img_from_url(url, img_path, verbose):\n",
    "            # Animated images can't be opened by cv2, which is used by \n",
    "            # ImageSequence. In future Pillow may be better choice.\n",
    "            try: \n",
    "                img = cv2.imread(img_path)\n",
    "                assert img is not None, 'Image must not be None.'\n",
    "            except Exception as e:\n",
    "                if verbose: print(f'IMAGE OPEN ERROR: {fname}')\n",
    "                os.remove(img_path)\n",
    "                continue\n",
    "            posts.append((url, post.permalink))\n",
    "\n",
    "    # Save csv with image and post urls.\n",
    "    df = pd.DataFrame(posts, columns=['url', 'post_url'])\n",
    "    df['post_url'] = 'https://www.reddit.com' + df.post_url\n",
    "    df.to_csv(os.path.join(out_dir, 'urls.csv'), index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scrapes images from a subreddit using the PRAW library, which imposes a maximum of 1,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def scrape_subreddit_images(sub, root_dir=None, steps=None, step_size=1_000, \n",
    "                            after_utc=None, verbose=True, print_freq=50):\n",
    "    \"\"\"Scrape reddit images using the pushshift api. This is sometimes\n",
    "    necessary because PRAW limits us to 1,000 images. Here, we still have to \n",
    "    collect <=1000 images per api call but we can sort and filter by timestamp\n",
    "    to accumulate all posts in the subreddit.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sub: str\n",
    "        Name of subreddit to scrape.\n",
    "    root_dir: str\n",
    "        Output directory to save results in. This will contain a subdirectory\n",
    "        'images' containing images and a text file 'urls.csv' in the root.\n",
    "        If None is specified, defaults to {sub}_images/.\n",
    "    steps: int or None\n",
    "        Number of pushshift api calls to make, where each call contains \n",
    "        `step_size` posts. Default is None which lets us get all posts in the\n",
    "        subreddit.\n",
    "    step_size: int\n",
    "        Number of posts per api call. Max available is 1,000, but when testing\n",
    "        we may want to use a smaller number.\n",
    "    verbose: bool\n",
    "        If True, print error messages when applicable. This is separate from \n",
    "        the print_freq functionality, which is concerned with updates rather\n",
    "        than errors.\n",
    "    print_freq: int\n",
    "        Frequency with which to print messages mentioning which image is being\n",
    "        scraped.\n",
    "    \"\"\"\n",
    "    root_dir = root_dir or sub + '_images'\n",
    "    img_dir = os.path.join(root_dir, 'test', sub)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    # Use logger to track last timestamp scraped in case process fails midway.\n",
    "    logging.basicConfig(handlers=[\n",
    "            logging.FileHandler(os.path.join(root_dir, 'scrape.log'), \n",
    "            mode='w')\n",
    "        ], \n",
    "        format='%(message)s',\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    curr_url = push_url = (f'http://api.pushshift.io/reddit/search/submission/'\n",
    "                           f'?subreddit={sub}&size={step_size}&sort_type='\n",
    "                           f'created_utc&sort=asc')\n",
    "    if after_utc: latest = after_utc\n",
    "    \n",
    "    metadata = []\n",
    "    i = 1\n",
    "    n_scraped = 0\n",
    "    while True:\n",
    "        if i > 1 or after_utc: curr_url = f'{push_url}&after={latest}'\n",
    "        if verbose: print(f'\\nPushshift call #{i}...')\n",
    "        \n",
    "        # Get dict of posts from pushshift.\n",
    "        r = requests.get(curr_url)\n",
    "        if r.status_code != 200: break\n",
    "        data = r.json()['data']\n",
    "        # When no more posts are found, data is an empty list.\n",
    "        if not data: break\n",
    "\n",
    "        # Iterate over each post returned from 1 api call.    \n",
    "        latest = data[-1]['created_utc']\n",
    "        for post in data:\n",
    "            img_url = post['url']\n",
    "            if is_imgur_page(img_url): img_url = fetch_imgur_url(img_url)\n",
    "            if not img_url: continue\n",
    "                \n",
    "            # Confirm image is fetched, then confirm file is not empty/corrupt.\n",
    "            fname = re.sub('\\?\\d*$', '', url.strip('/')).split('/')[-1]\n",
    "            img_path = os.path.join(img_dir, fname)\n",
    "            if fetch_img_from_url(img_url, img_path, verbose):\n",
    "                # Animated images can't be opened by cv2, which is used by \n",
    "                # ImageSequence. In future Pillow may be better choice.\n",
    "                try: \n",
    "                    img = cv2.imread(img_path)\n",
    "                    assert img is not None, 'Image must not be None.'\n",
    "                except Exception as e:\n",
    "                    if verbose: print(f'IMAGE OPEN ERROR: {str(e)} ({fname})')\n",
    "                    os.remove(img_path)\n",
    "                    continue\n",
    "                    \n",
    "                # Only store for images that were successfully saved.\n",
    "                metadata.append((post['full_link'], img_url))\n",
    "                logger.info(post['created_utc'])\n",
    "                if n_scraped % print_freq == 0: \n",
    "                    print(f'>>> Scraping image #{n_scraped}: {img_url}')\n",
    "                n_scraped += 1\n",
    "\n",
    "        if i == steps: break\n",
    "        i += 1\n",
    "    \n",
    "    df_kwargs = dict(path_or_buf=os.path.join(root_dir, 'urls.csv'), \n",
    "                     index=False)\n",
    "    if after_utc:\n",
    "        df_kwargs['mode'] = 'a'\n",
    "        df_kwargs['header'] = False\n",
    "    df = pd.DataFrame(metadata, columns=['post_url', 'img_url'])\n",
    "    df.to_csv(**df_kwargs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scrapes images using the Pushift API, which lets us scrape sequentially to collect as many images as we want. Eventually this should probably be combined with the PRAW scraping function into 1 higher level call where API choice happens behind the scenes, but for now this remains an acceptable (if clunky) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def remove_corrupt_images(dir_, mode='move', cv=True, verbose=True):\n",
    "    \"\"\"This can be used to remove empty, corrupt, or animated images \n",
    "    from a directory. Note that some images can be opened by Pillow but\n",
    "    not cv2, which is what ImageSequence uses to load images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_: str\n",
    "        Directory of images.\n",
    "    mode: str\n",
    "        'move' to move images 1 directory up (safer). 'remove' to remove \n",
    "        images.\n",
    "    cv: bool\n",
    "        True to use cv2, False to use Pillow. Pillow can open animated gifs\n",
    "        and cv2 can't. Keep in mind spellotape.dl.ImageSequence uses cv2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int: Number of images moved or deleted.\n",
    "    \"\"\"\n",
    "    def test_cv2(path):\n",
    "        img = cv2.imread(path)\n",
    "        assert img is not None, 'Image must not be None.'\n",
    "        \n",
    "    def test_pillow(path):\n",
    "        img = Image.open(path)\n",
    "    \n",
    "    open_img = test_cv2 if cv else test_pillow    \n",
    "    changed = 0\n",
    "    for i, path in enumerate(glob.glob(os.path.join(dir_, '*'))):\n",
    "        if i % 50 == 0: print(i)\n",
    "        try:\n",
    "            open_img(path)\n",
    "        except Exception as e:\n",
    "            if verbose: print('error:', e, 'path:', path)\n",
    "            if mode == 'move':\n",
    "                path_obj = Path(path)\n",
    "                os.rename(path, \n",
    "                          os.path.join(path_obj.parents[1], path_obj.name))\n",
    "            elif mode == 'remove':\n",
    "                os.remove(path)\n",
    "            changed += 1\n",
    "    \n",
    "    return changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to remove empty, corrupt, or animated images from a directory. This ensures a clean set of images so we can see how much data we will actually train or evaluate on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
