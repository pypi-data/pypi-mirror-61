{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from htools import save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML\n",
    "\n",
    "The machine learning module primarily provides utilities for working with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Debugger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transformer to be used to check the output of sklearn pipelines.\"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, path=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        verbose: bool\n",
    "            If True, print the shape and null count of the transformed feature\n",
    "            array.\n",
    "        path: str (Optional)\n",
    "            If specified, save the processed data to a pickle file. If left as\n",
    "            None, output will not be saved. Must use pkl file extension.\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        if path: os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(f'X shape: {X.shape}')\n",
    "            print(f'# of nulls: {np.isnan(X).sum()}')\n",
    "            print(pd.DataFrame(X[:5]))\n",
    "        if self.path: save(X, self.path)\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Debugger` class can be passed to sklearn pipelines to help debug preprocessing steps. We can choose to save the transformed data for closer analysis or simply view a few basic summary statistics and the first few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # export\n",
    "def build_pipeline(x, bin_cols=None, num_cols=None, cat_cols=None,\n",
    "                    scaler=None, num_impute='median'):\n",
    "    \"\"\"Wrapper to build scikit-learn pipelines that impute missing values,\n",
    "    scale numeric columns, and one hot encode categorical variables. A column\n",
    "    transformer is returned. No classifier is passed in, so fit and transform\n",
    "    are available methods but predict is not.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    x: pd.DataFrame\n",
    "        (Optional) A dataframe of features. Lists of numeric and categorical\n",
    "        features will be generated automatically based on dtypes.\n",
    "    scaler: sklearn.preprocessing.data\n",
    "        A scikit-learn scaler object (already instantiated). If none is\n",
    "        provided, mean normalization will be used.\n",
    "    num_impute: str\n",
    "        Strategy to use when imputing missing values in numeric (continuous)\n",
    "        features. One of ('mean', 'median', 'most_frequent'), default 'median'.\n",
    "        Note that categorical features will be automatically imputed with\n",
    "        'most_frequent'.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    sklearn.compose._column_transformer.ColumnTransformer\n",
    "    \"\"\"\n",
    "    scaler = scaler or StandardScaler()\n",
    "\n",
    "    # Dummy cols need no additional processing.\n",
    "    bin_cols = bin_cols or [col for col in x.columns\n",
    "                            if col.startswith('contains_') \n",
    "                            or col.startswith('is_')]\n",
    "    num_cols = num_cols or [col for col in x.select_dtypes(np.number).columns\n",
    "                            if col not in bin_cols]\n",
    "    cat_cols = cat_cols or x.select_dtypes('category').columns.tolist()\n",
    "\n",
    "    # Process continuous and categorical variables separately.\n",
    "    bin_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'))\n",
    "    num_pipe = make_pipeline(SimpleImputer(strategy=num_impute),\n",
    "                             scaler)\n",
    "    cat_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                             OneHotEncoder(drop='first'))\n",
    "\n",
    "    # Combine into single transformer and add classifier to end of pipeline.\n",
    "    return make_column_transformer((bin_pipe, bin_cols),\n",
    "                                   (num_pipe, num_cols),\n",
    "                                   (cat_pipe, cat_cols),\n",
    "                                   remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_pipeline` lets us easily handle preprocessing for continuous, categorical, and dummy variables all at once. It creates a separate pipeline for each type and passes each of them to a column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_gs_results(results, metric, sort_param=None):\n",
    "    \"\"\"Parse dictionary returned by grid search cv_results_ so we can examine\n",
    "    how metrics varied depending on hyperparameter choices.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    results: dict\n",
    "        cv_results_ dict.\n",
    "    sort_param: str\n",
    "        Hyperparameter to sort by.\n",
    "    metric: str\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    sort_by = [metric]\n",
    "    if sort_param:\n",
    "        sort_by.insert(0, sort_param)\n",
    "        \n",
    "    return pd.concat([pd.DataFrame(data['params']), \n",
    "                      pd.DataFrame(data[f'mean_test_{metric}'], \n",
    "                                   columns=[metric])],\n",
    "                     axis=1).sort_values(sort_by, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_gs_results` lets us quickly examine the results of a grid search and identify what range of hyperparameters may be useful to explore further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
