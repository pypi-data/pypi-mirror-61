{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of the different over-sampling algorithms\n\n\nThe following example attends to make a qualitative comparison between the\ndifferent over-sampling algorithms available in the imbalanced-learn package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import LinearSVC\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC,\n                                    KMeansSMOTE)\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.base import BaseSampler\n\nprint(__doc__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to create toy dataset. It using the\n``make_classification`` from scikit-learn but fixing some parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_dataset(n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3,\n                   class_sep=0.8, n_clusters=1):\n    return make_classification(n_samples=n_samples, n_features=2,\n                               n_informative=2, n_redundant=0, n_repeated=0,\n                               n_classes=n_classes,\n                               n_clusters_per_class=n_clusters,\n                               weights=list(weights),\n                               class_sep=class_sep, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the sample space after resampling\nto illustrate the characterisitic of an algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_resampling(X, y, sampling, ax):\n    X_res, y_res = sampling.fit_resample(X, y)\n    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n    # make nice plotting\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    return Counter(y_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will be used to plot the decision function of a\nclassifier given some data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Illustration of the influence of the balancing ratio\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first illustrate the influence of the balancing ratio on some toy\ndata using a linear SVM classifier. Greater is the difference between the\nnumber of samples in each class, poorer are the classfication results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\nax_arr = (ax1, ax2, ax3, ax4)\nweights_arr = ((0.01, 0.01, 0.98), (0.01, 0.05, 0.94),\n               (0.2, 0.1, 0.7), (0.33, 0.33, 0.33))\nfor ax, weights in zip(ax_arr, weights_arr):\n    X, y = create_dataset(n_samples=1000, weights=weights)\n    clf = LinearSVC().fit(X, y)\n    plot_decision_function(X, y, clf, ax)\n    ax.set_title('Linear SVC with y={}'.format(Counter(y)))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random over-sampling to balance the data set\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random over-sampling can be used to repeat some samples and balance the\nnumber of samples between the dataset. It can be seen that with this trivial\napproach the boundary decision is already less biaised toward the majority\nclass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\nX, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\npipe = make_pipeline(RandomOverSampler(random_state=0), LinearSVC())\npipe.fit(X, y)\nplot_decision_function(X, y, pipe, ax2)\nax2.set_title('Decision function for RandomOverSampler')\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More advanced over-sampling using ADASYN and SMOTE\n##############################################################################\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of repeating the same samples when over-sampling, we can use some\nspecific heuristic instead. ADASYN and SMOTE can be used in this case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make an identity sampler\nclass FakeSampler(BaseSampler):\n\n    _sampling_type = 'bypass'\n\n    def _fit_resample(self, X, y):\n        return X, y\n\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\nX, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\nsampler = FakeSampler()\nclf = make_pipeline(sampler, LinearSVC())\nplot_resampling(X, y, sampler, ax1)\nax1.set_title('Original data - y={}'.format(Counter(y)))\n\nax_arr = (ax2, ax3, ax4)\nfor ax, sampler in zip(ax_arr, (RandomOverSampler(random_state=0),\n                                SMOTE(random_state=0),\n                                ADASYN(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_resampling(X, y, sampler, ax)\n    ax.set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following plot illustrate the difference between ADASYN and SMOTE. ADASYN\nwill focus on the samples which are difficult to classify with a\nnearest-neighbors rule while regular SMOTE will not make any distinction.\nTherefore, the decision function depending of the algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nX, y = create_dataset(n_samples=10000, weights=(0.01, 0.05, 0.94))\n\nclf = LinearSVC().fit(X, y)\nplot_decision_function(X, y, clf, ax1)\nax1.set_title('Linear SVC with y={}'.format(Counter(y)))\nsampler = SMOTE()\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax2)\nax2.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nsampler = ADASYN()\nclf = make_pipeline(sampler, LinearSVC())\nclf.fit(X, y)\nplot_decision_function(X, y, clf, ax3)\nax3.set_title('Decision function for {}'.format(sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to those sampling particularities, it can give rise to some specific\nissues as illustrated below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 15))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\nax_arr = ((ax1, ax2), (ax3, ax4))\nfor ax, sampler in zip(ax_arr, (SMOTE(random_state=0),\n                                ADASYN(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(\n        sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SMOTE proposes several variants by identifying specific samples to consider\nduring the resampling. The borderline version will detect which point to\nselect which are in the border between two classes. The SVM version will use\nthe support vectors found using an SVM algorithm to create new sample while\nthe KMeans version will make a clustering before to generate samples in each\ncluster independently depending each cluster density.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ((ax1, ax2), (ax3, ax4),\n      (ax5, ax6), (ax7, ax8),\n      (ax9, ax10)) = plt.subplots(5, 2, figsize=(15, 30))\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94),\n                      class_sep=0.8)\n\n\nax_arr = ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10))\nfor ax, sampler in zip(ax_arr,\n                       (SMOTE(random_state=0),\n                        BorderlineSMOTE(random_state=0, kind='borderline-1'),\n                        BorderlineSMOTE(random_state=0, kind='borderline-2'),\n                        KMeansSMOTE(random_state=0),\n                        SVMSMOTE(random_state=0))):\n    clf = make_pipeline(sampler, LinearSVC())\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax[0])\n    ax[0].set_title('Decision function for {}'.format(\n        sampler.__class__.__name__))\n    plot_resampling(X, y, sampler, ax[1])\n    ax[1].set_title('Resampling using {}'.format(sampler.__class__.__name__))\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When dealing with a mixed of continuous and categorical features, SMOTE-NC\nis the only method which can handle this case.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a synthetic data set with continuous and categorical features\nrng = np.random.RandomState(42)\nn_samples = 50\nX = np.empty((n_samples, 3), dtype=object)\nX[:, 0] = rng.choice(['A', 'B', 'C'], size=n_samples).astype(object)\nX[:, 1] = rng.randn(n_samples)\nX[:, 2] = rng.randint(3, size=n_samples)\ny = np.array([0] * 20 + [1] * 30)\n\nprint('The original imbalanced dataset')\nprint(sorted(Counter(y).items()))\nprint('The first and last columns are containing categorical features:')\nprint(X[:5])\n\nsmote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nprint('Dataset after resampling:')\nprint(sorted(Counter(y_resampled).items()))\nprint('SMOTE-NC will generate categories for the categorical features:')\nprint(X_resampled[-5:])\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}