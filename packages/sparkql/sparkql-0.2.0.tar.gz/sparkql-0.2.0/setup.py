# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['sparkql', 'sparkql.fields']

package_data = \
{'': ['*']}

install_requires = \
['pyspark>=2.4.1,<3.0.0']

extras_require = \
{':python_version >= "3.6" and python_version < "3.7"': ['dataclasses>=0.7.0,<0.8.0']}

entry_points = \
{'console_scripts': ['find-releasable-changes = tasks:find_releasable_changes',
                     'lint = tasks:lint',
                     'prepare-release = tasks:prepare_release',
                     'reformat = tasks:reformat',
                     'test = tasks:test',
                     'typecheck = tasks:typecheck',
                     'verify-all = tasks:verify_all']}

setup_kwargs = {
    'name': 'sparkql',
    'version': '0.2.0',
    'description': 'sparkql: Apache Spark SQL DataFrame schema management for sensible humans',
    'long_description': '# sparkql âœ¨\n\n[![PyPI version](https://badge.fury.io/py/sparkql.svg)](https://badge.fury.io/py/sparkql)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![CircleCI](https://circleci.com/gh/mattjw/sparkql.svg?style=svg)](https://circleci.com/gh/mattjw/sparkql)\n\nPython Spark SQL DataFrame schema management for sensible humans.\n\n## Why use sparkql\n\n`sparkql` takes the pain out of working with DataFrame schemas in PySpark.\nIn general, it makes schema definition more Pythonic, and it\'s\nparticularly useful you\'re dealing with structured data.\n\nIn plain old PySpark, you might find that you write schemas\n[like this](./examples/conferences_comparison/plain_schema.py):\n\n```python\nCITY_SCHEMA = StructType()\nCITY_NAME_FIELD = "name"\nCITY_SCHEMA.add(StructField(CITY_NAME_FIELD, StringType(), False))\nCITY_LAT_FIELD = "latitude"\nCITY_SCHEMA.add(StructField(CITY_LAT_FIELD, FloatType()))\nCITY_LONG_FIELD = "longitude"\nCITY_SCHEMA.add(StructField(CITY_LONG_FIELD, FloatType()))\n\nCONFERENCE_SCHEMA = StructType()\nCONF_NAME_FIELD = "name"\nCONFERENCE_SCHEMA.add(StructField(CONF_NAME_FIELD, StringType(), False))\nCONF_CITY_FIELD = "city"\nCONFERENCE_SCHEMA.add(StructField(CONF_CITY_FIELD, CITY_SCHEMA))\n```\n\nAnd then plain old PySpark makes you deal with nested fields like this:\n\n```python\ndframe.withColumn("city_name", df[CONF_CITY_FIELD][CITY_NAME_FIELD])\n```\n\nInstead, with `sparkql`, schemas become a lot\n[more literate](./examples/conferences_comparison/sparkql_schema.py):\n\n```python\nclass City(Struct):\n    name = String(nullable=False)\n    latitude = Float()\n    longitude = Float()\n\nclass Conference(Struct):\n    name = String(nullable=False)\n    city = City()\n```\n\nAs does dealing with nested fields:\n\n```python\ndframe.withColumn("city_name", path_col(Conference.city.name))\n```\n\n## Features\n\n### Automated field naming\n\nBy default, field names are inferred from the attribute name in the\nstruct they are declared.\n\nFor example, given the struct\n\n```python\nclass Geolocation(Struct):\n    latitude = Float()\n    longitude = Float()\n```\n\nthe concrete name of the `Geolocation.latitude` field is `latitude`.\n\nNames also be overridden by explicitly specifying the field name as an\nargument to the field\n\n```python\nclass Geolocation(Struct):\n    latitude = Float("lat")\n    longitude = Float("lon")\n```\n\nwhich would mean the concrete name of the `Geolocation.latitude` field\nis `lat`.\n\n### Field paths, and nested objects\n\nReferencing fields in nested data can be a chore. `sparkql` simplifies this\nwith path referencing.\n\n[For example](./examples/nested_objects/sparkql_example.py), if we have a\nschema with nested objects:\n\n```python\nclass Address(Struct):\n    post_code = String()\n    city = String()\n\n\nclass User(Struct):\n    username = String(nullable=False)\n    address = Address()\n\n\nclass Comment(Struct):\n    message = String()\n    author = User(nullable=False)\n\n\nclass Article(Struct):\n    title = String(nullable=False)\n    author = User(nullable=False)\n    comments = Array(Comment())\n```\n\nWe can use `path_str` to turn a path into a Spark-understandable string:\n\n```python\nauthor_city_str = path_str(Article.author.address.city)\n"author.address.city"\n```\n\nFor paths that include an array, two approaches are provided:\n\n```python\ncomment_usernames_str = path_str(Article.comments.e.author.username)\n"comments.author.username"\n\ncomment_usernames_str = path_str(Article.comments.author.username)\n"comments.author.username"\n```\n\nBoth give the same result. However, the former (`e`) is more\ntype-oriented. The `e` attribute corresponds to the array\'s element\nfield. Although this looks strange at first, it has the advantage of\nbeing inspectable by IDEs and other tools, allowing goodness such as\nIDE auto-completion and IDE-assisted refactoring.\n\n`path_col` is the counterpart to `path_str` and returns a Spark `Column`\nobject for the path, allowing it to be used in all places where Spark\nrequires a column.\n\n### Composite schemas\n\nStructs can be re-used to build composite schemas with _inheritance_ or _includes_.\n\n#### Using inheritance\n\nFor [example](./examples/composite_schemas/inheritance.py), the following:\n\n```python\nclass BaseEvent(Struct):\n    correlation_id = String(nullable=False)\n    event_time = Timestamp(nullable=False)\n\nclass RegistrationEvent(BaseEvent):\n    user_id = String(nullable=False)\n```\n\nwill produce the `RegistrationEvent` schema:\n\n```text\nStructType(List(\n    StructField(correlation_id,StringType,false),\n    StructField(event_time,TimestampType,false),\n    StructField(user_id,StringType,false)))\n```\n\n#### Using an `includes` declaration\n\nFor [example](./examples/composite_schemas/includes.py), the following:\n\n```python\nclass EventMetadata(Struct):\n    correlation_id = String(nullable=False)\n    event_time = Timestamp(nullable=False)\n\nclass RegistrationEvent(Struct):\n    class Meta:\n        includes = [EventMetadata]\n    user_id = String(nullable=False)\n```\n\nwill produce the `RegistrationEvent` schema:\n\n```text\nStructType(List(\n    StructField(user_id,StringType,false),\n    StructField(correlation_id,StringType,false),\n    StructField(event_time,TimestampType,false)))\n```\n\n### Prettified Spark schema strings\n\nSpark\'s stringified schema representation isn\'t very user friendly, particularly for large schemas:\n\n\n```text\nStructType(List(StructField(name,StringType,false),StructField(city,StructType(List(StructField(name,StringType,false),StructField(latitude,FloatType,true),StructField(longitude,FloatType,true))),true)))\n```\n\nThe function `pretty_schema` will return something more useful:\n\n```text\nStructType(List(\n    StructField(name,StringType,false),\n    StructField(city,\n        StructType(List(\n            StructField(name,StringType,false),\n            StructField(latitude,FloatType,true),\n            StructField(longitude,FloatType,true))),\n        true)))\n```\n\n## Contributing\n\nContributions are very welcome. Developers who\'d like to contribute to\nthis project should refer to [CONTRIBUTING.md](./CONTRIBUTING.md).\n',
    'author': 'Matt J Williams',
    'author_email': 'mattjw@mattjw.net',
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://github.com/mattjw/sparkql',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'entry_points': entry_points,
    'python_requires': '>=3.6,<3.8',
}


setup(**setup_kwargs)
