#!/usr/bin/env python

"""
Convert a chunkify training file
"""

import os
import h5py
import random
import numpy as np
from argparse import ArgumentParser
from collections import defaultdict
from bisect import bisect_left, insort
from itertools import chain, zip_longest



def align(samples, pointers):
    """ align to the start and end of the mapping """
    return samples[pointers[0]:pointers[-1]], pointers - pointers[0]


def scale(read, samples, normalise=True):
    """ scale and normalise a read """
    scaling = read.attrs['range'] / read.attrs['digitisation']
    scaled = (scaling * (samples + read.attrs['offset'])).astype(np.float32)
    if normalise:
        return (scaled - read.attrs['shift_frompA']) / read.attrs['scale_frompA']
    return scaled


def num_reads(tfile):
    """ return the sample lengths for each read in the training file """
    with h5py.File(tfile, 'r') as training_file:
        return len(training_file['Reads'])


def match_reads(tfile, dims=2):
    """ match nD reads """
    rids = defaultdict(list)
    with h5py.File(tfile) as f:
        for read in f['Reads']:
            insort(rids[read[:-2]], read[-2:])

    unmatched = []
    for r, c in rids.items():
        if len(c) != dims:
            unmatched.append(r)

    for u in unmatched:
        del rids[u]

    return rids



def get_reads(tfile):
    """ get each dataset per read """

    dims = 2
    rids = match_reads(tfile)

    with h5py.File(tfile, 'r') as training_file:

        #for read_id in np.random.permutation(training_file['Reads']):

        for read_id, suf in rids.items():

            # template
            read = training_file['Reads/%s%s' % (read_id, suf[0])]

            reference = read['Reference'][:]
            pointers = read['Ref_to_signal'][:]

            samples = read['Dacs'][:]
            samples = scale(read, samples)
            samples, pointers = align(samples, pointers)

            # complement
            read_c = training_file['Reads/%s%s' % (read_id, suf[1])]

            reference_c = read_c['Reference'][:]
            pointers_c = read_c['Ref_to_signal'][:]

            samples_c = read_c['Dacs'][:]
            samples_c = scale(read_c, samples_c)
            samples_c, pointers_c = align(samples_c, pointers_c)

            # todo: align refs properly

            yield read_id, samples, samples_c, reference, pointers, pointers_c, reference_c


def main(args):

    random.seed(args.seed)
    np.random.seed(args.seed)
    os.makedirs(args.output_directory, exist_ok=True)

    read_idx = 0
    chunk_idx = 0
    chunk_count = 0

    min_bases = 0
    max_bases = 0
    off_the_end_ref = 0
    off_the_end_sig = 0

    comp_ref = 0

    total_reads = num_reads(args.chunkify_file)

    dims = 2

    chunks = np.zeros((args.chunks, dims, args.max_seq_len * args.max_samples_per_base), dtype=np.float32)
    chunk_lengths = np.zeros(args.chunks, dtype=np.uint16)

    targets = np.zeros((args.chunks, args.max_seq_len), dtype=np.uint8)
    target_lengths = np.zeros(args.chunks, dtype=np.uint16)

    for read_id, samples, samples_c, reference, pointers, pointers_c, reference_c in get_reads(args.chunkify_file):

        read_idx += 1
        num_samples = np.int32(len(reference) * args.samples_per_read)

        sequence_length = len(reference)
        squiggle_duration = len(samples)

        seq_starts = np.random.randint(0, sequence_length, size=num_samples)
        seq_ends   = np.random.randint(args.min_seq_len, args.max_seq_len, size=num_samples) + seq_starts

        for start, end in zip(seq_starts, seq_ends):

            chunk_idx += 1

            if end > sequence_length:
                off_the_end_ref += 1
                continue

            squiggle_start = pointers[start]
            squiggle_end = pointers[end]
            squiggle_length = squiggle_end - squiggle_start

            if end > len(pointers_c):
                comp_ref += 1
                continue

            # c
            squiggle_start_c = pointers_c[-end]
            squiggle_end_c = pointers_c[-start]
            squiggle_length_c = squiggle_end_c - squiggle_start_c

            reference_length = end - start

            samples_per_base = squiggle_length / reference_length

            samples_per_base_c = squiggle_length_c / reference_length

            if samples_per_base < args.min_samples_per_base or samples_per_base_c < args.min_samples_per_base:
                min_bases += 1
                continue

            if samples_per_base > args.max_samples_per_base or samples_per_base_c > args.max_samples_per_base:
                max_bases += 1
                continue

            if squiggle_end > squiggle_duration or squiggle_end_c > len(samples_c):
                off_the_end_sig += 1
                continue

            chunks[chunk_count, 0, :squiggle_length] = samples[squiggle_start:squiggle_end]

            # todo: register better
            chunks[chunk_count, 1, :squiggle_length_c] = samples_c[squiggle_start_c:squiggle_end_c][::-1]

            chunk_lengths[chunk_count] = squiggle_length

            # index alphabet from 1 (ctc blank labels - 0)
            targets[chunk_count, :reference_length] = reference[start:end] + 1
            target_lengths[chunk_count] = reference_length

            chunk_count += 1

            if chunk_count == args.chunks:
                break

        if chunk_count == args.chunks:
            break

    skipped = chunk_idx - chunk_count
    percent = (skipped / chunk_count * 100) if skipped else 0

    print("Processed %s reads of out %s [%.2f%%]" % (read_idx, total_reads, read_idx / total_reads * 100))
    print("Skipped %s chunks out of %s due to bad chunks [%.2f%%].\n" % (skipped, chunk_count, percent))
    print("Reason for skipping:")
    print("  - off the end (signal)          ", off_the_end_sig)
    print("  - off the end (sequence)        ", off_the_end_ref)
    print("  - minimum number of bases       ", min_bases)
    print("  - maximum number of bases       ", max_bases)
    print("  - complement too short          ", comp_ref)

    if chunk_count < args.chunks:
        chunks = np.delete(chunks, np.s_[chunk_count:], axis=0)
        chunk_lengths = chunk_lengths[:chunk_count]
        targets = np.delete(targets, np.s_[chunk_count:], axis=0)
        target_lengths = target_lengths[:chunk_count]

    np.save(os.path.join(args.output_directory, "chunks.npy"), chunks)
    np.save(os.path.join(args.output_directory, "chunk_lengths.npy"), chunk_lengths)
    np.save(os.path.join(args.output_directory, "references.npy"), targets)
    np.save(os.path.join(args.output_directory, "reference_lengths.npy"), target_lengths)

    print()
    print("Training data written to %s:" % args.output_directory)
    print("  - chunks.npy with shape", chunks.shape)
    print("  - chunk_lengths.npy with shape", chunk_lengths.shape)
    print("  - references.npy with shape", targets.shape)
    print("  - reference_lengths.npy shape", target_lengths.shape)


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument("chunkify_file")
    parser.add_argument("output_directory")
    parser.add_argument("--seed", default=25, type=int)
    parser.add_argument("--chunks", default=1000000, type=int)
    parser.add_argument("--min-seq-len", default=128, type=int)
    parser.add_argument("--max-seq-len", default=256, type=int)
    parser.add_argument("--samples-per-read", default=0.05, type=float)
    parser.add_argument("--min-samples-per-base", default=8, type=int)
    parser.add_argument("--max-samples-per-base", default=16, type=int)
    main(parser.parse_args())
