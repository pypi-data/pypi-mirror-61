# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_connector.ipynb (unless otherwise specified).

__all__ = ['LocalLog', 'StreamLog', 'DBConnector', 'DatabaseConnection', 'move_table', 'current_datetime_dict',
           'NativeTemplateEnvironment']

# Cell

import sys
import os
import datetime
import time
import logging
import calendar
import glob
import json
from io import StringIO
import configparser

import psycopg2
import psycopg2.extras as extras
import pandas as pd

from . import s3funcs

class LocalLog(object):
    """
    Creates a local logfile where these classes are imported,
    showing database connections and what was run on databases.
    """

    def __init__(self, logname, log_path):
        self.logger = logging.getLogger(logname)
        self.logger.setLevel(logging.DEBUG)
        # Create a logfile in log_path.
        self.fh = logging.FileHandler((log_path + logname + ".log").replace("//", "/"))
        self.fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '{"time": %(asctime)s,"name" : %(name)s,'
            '"level" : %(levelname)s, "detail" : %(message)s}'
        )
        self.fh.setFormatter(formatter)
        self.logger.addHandler(self.fh)

    def debug(self, message):
        self.logger.debug(message)

    def info(self, message):
        self.logger.info(message)


class StreamLog(object):
    """
    Logs to STDOUT, useful for scripts that query DBs.
    Every query will print JSON with information about SQL ran.
    """

    def __init__(self, logname):
        self.logger = logging.getLogger(logname)
        self.logger.setLevel(logging.DEBUG)
        self.fh = logging.StreamHandler(sys.stdout)
        self.fh.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '{"time": %(asctime)s,"name" : %(name)s,'
            '"level" : %(levelname)s, "detail" : %(message)s}'
        )
        self.fh.setFormatter(formatter)
        self.logger.addHandler(self.fh)

    def debug(self, message):
        print(message)

    def info(self, message):
        print(message)


class DBConnector(object):
    """
    Object that stores connections to many databases, which are created
    from a .ini file that is passed as an object.
    """

    def __init__(self, config_file, log=StreamLog("default-stream-log")):
        filetype = config_file.split(".")[-1]
        if filetype == "ini":
            self.config = configparser.RawConfigParser()
            self.config.read(config_file)
        if filetype == "json":
            with open(config_file, "r") as f:
                self.config = json.loads(f.read())
        self.log = log
        self.active_connections = {}
        # Run func and initialize objects on self
        self._init_database_objects()

    def _init_database_objects(self):
        criteria = ["port", "database", "user", "hostname"]
        # review sections in config for db sections
        for section in self.config.sections():
            n = self.config.options(section)
            # if the section has the criteria, store them
            if set(criteria).issubset(set(n)):
                bkt = {}
                for i in criteria:
                    p = self.config.get(section, i)
                    bkt[i] = p
                # allows db.section referencing
                setattr(
                    self, section.replace("-", ""), DatabaseConnection(bkt, self.log)
                )
                self.active_connections[section] = DatabaseConnection(bkt, self.log)
        # Check if configuration file has proper sections
        if len(self.active_connections) == 0:
            raise Exception(
                """\n
                Configuration file contained no Postgres sections. Please review
                and format like below example :
                    [db_shortname]
                    hostname = host.com
                    port = 5432
                    database = dbname (default e.g postgres)
                    user = htpeter (thats my name :))
                """
            )

    def close_all_conns(self):
        counter = 0
        conns = []
        for i in self.active_connections.keys():
            adj = i.replace("-", "")
            obj = getattr(self, adj)
            try:
                obj.conn.close()
                conns.append(adj)
                counter += 1
            except Exception as e:
                pass
        print("\n\t Closed {0} connections.\n\t {1} ".format(str(counter), str(conns)))

    def __repr__(self):
        from pprint import pformat

        return pformat(self.active_connections, indent=4)

    def __getitem__(self, item):
        return self.active_connections[item]


class DatabaseConnection(object):
    """
    Built on psycopg2 connections, provides high level API
    for running queries and returning necessary information.
    Implemented as an object on DBConnector.
    """

    def __init__(self, credentials, log):
        self.cred = credentials
        self.log = log

    def _connect(self):
        try:
            conn = psycopg2.connect(
                host=self.cred["hostname"],
                port=self.cred["port"],
                database=self.cred["database"],
                user=self.cred["user"],
                cursor_factory=extras.RealDictCursor,
            )
            self.log.info("Connected to {}".format(self.cred["hostname"]))
            return conn
        except Exception as e:
            self.log.debug(e)

    def _execute(self, sql, commit=False):
        """
        Private function focused on handling the nitty gritty of running a psycopg2
        execute and returning the results based on "commit" boolean.
        """
        if commit == False:
            try:
                # Time the query and get results, log info
                cur = self.conn.cursor()
                t0 = time.time()
                try:
                    cur.execute(sql)
                except Exception as e:
                    self.conn.rollback()
                    cur.execute(sql)
                runtime = time.time() - t0
                resp = cur.fetchall()
                cur.close()
                detail = '{{"database" : "{0}","runtime" : {2},\n "sql" : "{1}"}}'.format(
                    self.cred["hostname"], sql, str(runtime)
                )
                self.log.info(detail)
                return resp

            except Exception as e:
                detail = '{{"database" : {0},"error" : "{2}",\n "sql" : "{1}"}}'.format(
                    self.cred["hostname"], sql, e
                )
                self.log.debug(detail)
                return e
        else:
            try:
                # Time the query and get the ROWCOUNT, log info
                cur = self.conn.cursor()
                t0 = time.time()
                try:
                    cur.execute(sql)
                except Exception as e:
                    self.conn.rollback()
                    cur.execute(sql)
                runtime = time.time() - t0
                self.conn.commit()
                resp = str(cur.rowcount)
                detail = '{{"database" : "{0}","row_count" : {2}, "runtime" : {3},\n "sql" : "{1}"}}'.format(
                    self.cred["hostname"], sql, resp, str(runtime)
                )
                self.log.info(detail)
                cur.close()
                return resp

            except Exception as e:
                detail = '{{"database" : "{0}", "error" : "{2}",\n "sql" : "{1}"}}'.format(
                    self.cred["hostname"], sql, e
                )
                self.log.debug(detail)
                return e

    def get_pg_cursor(self, cursor_type=None):
        conn = self._connect()
        return conn, conn.cursor(cursor_factory=cursor_type)

    def qry(self, sql, commit=False):
        """
        Runs a query. Creates connection if non-existent.
        """
        if hasattr(self, "conn"):
            try:
                resp = self._execute(sql, commit)
                if commit: return resp
                else: return pd.DataFrame(resp)
            except ValueError as e:
                print(sql)
                raise Exception(e)

        else:
            self.conn = self._connect()
            resp = self._execute(sql, commit)
            if commit:
                return True
            else:
                return pd.DataFrame(resp)

    def write_df_to_table(self,
                          dataframe: pd.DataFrame,
                          create_statement: str,
                          schema: str,
                          tablename: str,
                          columns: tuple,
                          db_engine: str,
                          s3_bucket: str,
                          append = True) -> int:
        """
        Writes a dataframe to the target table using the create_statement.
        Its best practice to add CREATE TABLE IF NOT EXISTS to your statement.
        DROPS should be done seperately and explicity from this process.
        """
        # create the table if it does not exist
        if not self.qry(create_statement, commit = True):
            raise Exception(f'Error writing {schema}.{tablename} to {self.cred["hostname"]}')
        # delete table results if append does not exist
        if not append:
            self.qry(f"DELETE FROM {schema}.{tablename}", commit = True)

        if db_engine == 'postgres':
            # prepare dataframe for efficent write
            stream = StringIO()
            dataframe.to_csv(stream, sep="\t", header=False, index=False, float_format='%.0f')
            # move to beginning of stream
            stream.seek(1)
            contents = stream.getvalue()
            # create psycopg2 cursor
            cur = self.conn.cursor()
            try:
                cur.copy_from(
                    stream,
                    f'{schema}.{tablename}',
                    null="",
                    columns=columns
                )
                self.conn.commit()
            except psycopg2.errors.InFailedSqlTransaction as e:
                self.conn.rollback()
                raise Exception(f'Error copying to {schema}.{tablename}. Transaction block failed.')
            # close cursor after job is complete
            cur.close()
            return True
        elif db_engine == 'redshift':
            # write dataframe to S3 bucket, also get credentials
            bucket = s3funcs.S3Bucket(s3_bucket)
            bucket.write_dataframe(dataframe, f'atb-writer/{tablename}.tsv')
            access_key = bucket.aws_access_key_id
            secret_access_key = bucket.aws_secret_access_key
            # copy
            load = f"""
            COPY {schema}.{tablename}
            FROM 's3://{s3_bucket}/atb-writer/{tablename}.tsv'
            CREDENTIALS 'aws_access_key_id={access_key};aws_secret_access_key={secret_access_key}'
            DELIMITER '\t';
            """
            self.qry(load, commit = True)
            # delete from s3
            bucket.bucket.Object("atb-writer/{tablename}.tsv").delete()
            return True


def move_table(table, from_db, to_db):
    assert type(from_db) == DatabaseConnection
    assert type(to_db) == DatabaseConnection
    creds = {
        "from_host": from_db.cred["hostname"],
        "from_db": from_db.cred["database"],
        "from_user": from_db.cred["user"],
        "to_host": to_db.cred["hostname"],
        "to_db": to_db.cred["database"],
        "to_user": to_db.cred["user"],
        "table": table,
    }
    os.system(
        "pg_dump -t {table} -h {from_host} -U {from_user} {from_db} | psql -h {to_host} -U {to_user} {to_db}".format(
            **creds
        )
    )


def current_datetime_dict(current_time=datetime.datetime.now()):
    cy = datetime.datetime.now().year
    cm = datetime.datetime.now().strftime("%m")
    now = current_time

    if now.month == 1:
        pm_d = datetime.datetime(now.year - 1, 12, 1)
        pm = datetime.datetime(now.year - 1, 12, 1).strftime("%m")
        mrange = calendar.monthrange(pm_d.year, pm_d.month)
        iso_mstart = str(datetime.datetime(pm_d.year, pm_d.month, 1))
        iso_mend = str(datetime.datetime(pm_d.year, pm_d.month, mrange[1]))
    else:
        pm_d = datetime.datetime(now.year, now.month - 1, 1)
        pm = datetime.datetime(now.year, now.month - 1, 1).strftime("%m")
        mrange = calendar.monthrange(pm_d.year, pm_d.month)
        iso_mstart = str(datetime.datetime(pm_d.year, pm_d.month, 1))
        iso_mend = str(datetime.datetime(pm_d.year, pm_d.month, mrange[1]))

    data = {
        "now": str(now),
        "past_iso_start": iso_mstart,
        "past_iso_end": iso_mend,
        "current_yearmo": str(now.year) + str(now.strftime("%m")),
        "past_yearmo": str(pm_d.year) + str(pm),
        "current_year_mm": str(now.year) + "-" + str(now.strftime("%m")),
        "past_year_mm": str(pm_d.year) + "-" + str(pm_d.strftime("%m")),
    }

    return data


class NativeTemplateEnvironment(object):
    """
    Reads files from a folder, passed during initialization, and injects values
    from a dictionary. Usings Python native string functions.
    """

    def __init__(self, templates_folder):
        self.path = templates_folder

    def build(self, template_name, context):
        complete_path = self.path + "/" + template_name
        # In case user passes trailing / in self.path
        complete_path = complete_path.replace("//", "/")
        f = open(complete_path, "r")
        skeleton = f.read()
        f.close()
        sql = skeleton.format(**context)
        return sql

